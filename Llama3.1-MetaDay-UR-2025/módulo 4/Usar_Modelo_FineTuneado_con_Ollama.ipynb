{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ü¶ô Usar Modelo Fine-tuneado con Ollama\n",
        "\n",
        "## Meta Day Uruguay 2025 - Conexi√≥n M√≥dulo 4 ‚Üí M√≥dulo 5\n",
        "\n",
        "Este notebook muestra c√≥mo usar el modelo que fine-tuneaste en el notebook anterior con **Ollama** para crear un sistema de chat local.\n",
        "\n",
        "### Prerrequisitos:\n",
        "1. Haber completado el fine-tuning con Unsloth\n",
        "2. Tener los archivos GGUF exportados\n",
        "3. Ollama instalado localmente\n",
        "\n",
        "### Lo que aprender√°s:\n",
        "- Importar tu modelo fine-tuneado a Ollama\n",
        "- Crear un Modelfile personalizado\n",
        "- Configurar par√°metros de inferencia\n",
        "- Probar el modelo en un chat interactivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üîß Configuraci√≥n Inicial\n",
        "\n",
        "### Verificar instalaci√≥n de Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_ollama"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def run_command(command):\n",
        "    \"\"\"Ejecuta un comando y retorna el resultado\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "        return result.stdout.strip(), result.stderr.strip(), result.returncode\n",
        "    except Exception as e:\n",
        "        return \"\", str(e), 1\n",
        "\n",
        "# Verificar si Ollama est√° instalado\n",
        "stdout, stderr, code = run_command(\"ollama --version\")\n",
        "if code == 0:\n",
        "    print(f\"‚úÖ Ollama instalado: {stdout}\")\n",
        "else:\n",
        "    print(\"‚ùå Ollama no est√° instalado\")\n",
        "    print(\"üì• Descarga desde: https://ollama.ai\")\n",
        "    print(\"üí° O instala con: curl -fsSL https://ollama.ai/install.sh | sh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "modelfile"
      },
      "source": [
        "## üìù Crear Modelfile Personalizado\n",
        "\n",
        "Un Modelfile define c√≥mo Ollama debe cargar y configurar tu modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_modelfile"
      },
      "outputs": [],
      "source": [
        "# Configuraci√≥n del modelo\n",
        "model_name = \"llama3.1-metaday-finetune\"\n",
        "gguf_path = \"./gguf_model/model-q4_k_m.gguf\"  # Ajusta la ruta seg√∫n tu exportaci√≥n\n",
        "\n",
        "# Crear Modelfile\n",
        "modelfile_content = f\"\"\"# Modelo Llama 3.1 Fine-tuneado - Meta Day Uruguay 2025\n",
        "FROM {gguf_path}\n",
        "\n",
        "# Template de chat (ChatML)\n",
        "TEMPLATE \"\"\"<|im_start|>system\n",
        "{{ .System }}<|im_end|>\n",
        "<|im_start|>user\n",
        "{{ .Prompt }}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "# Par√°metros de inferencia optimizados\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER top_k 40\n",
        "PARAMETER repeat_penalty 1.1\n",
        "PARAMETER num_ctx 2048\n",
        "\n",
        "# Mensaje del sistema\n",
        "SYSTEM \"\"\"Eres un asistente de IA √∫til y conocedor, entrenado durante el Meta Day Uruguay 2025. \n",
        "Respondes de manera clara, precisa y educativa. Siempre intentas ser √∫til y proporcionar \n",
        "informaci√≥n valiosa al usuario.\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "# Guardar Modelfile\n",
        "with open(\"Modelfile\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(modelfile_content)\n",
        "\n",
        "print(\"‚úÖ Modelfile creado\")\n",
        "print(\"üìÑ Contenido:\")\n",
        "print(\"=\" * 50)\n",
        "print(modelfile_content)\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_model"
      },
      "source": [
        "## üì¶ Importar Modelo a Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_to_ollama"
      },
      "outputs": [],
      "source": [
        "# Verificar que el archivo GGUF existe\n",
        "if os.path.exists(gguf_path):\n",
        "    print(f\"‚úÖ Archivo GGUF encontrado: {gguf_path}\")\n",
        "    \n",
        "    # Crear el modelo en Ollama\n",
        "    print(f\"üì¶ Importando modelo '{model_name}' a Ollama...\")\n",
        "    stdout, stderr, code = run_command(f\"ollama create {model_name} -f Modelfile\")\n",
        "    \n",
        "    if code == 0:\n",
        "        print(\"üéâ ¬°Modelo importado exitosamente!\")\n",
        "        print(f\"üìù Salida: {stdout}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error al importar: {stderr}\")\n",
        "else:\n",
        "    print(f\"‚ùå Archivo GGUF no encontrado: {gguf_path}\")\n",
        "    print(\"üí° Aseg√∫rate de haber ejecutado la exportaci√≥n GGUF en el notebook anterior\")\n",
        "    print(\"üí° O ajusta la variable 'gguf_path' con la ruta correcta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "list_models"
      },
      "source": [
        "## üìã Verificar Modelos Disponibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_models"
      },
      "outputs": [],
      "source": [
        "# Listar modelos en Ollama\n",
        "print(\"üìã Modelos disponibles en Ollama:\")\n",
        "stdout, stderr, code = run_command(\"ollama list\")\n",
        "\n",
        "if code == 0:\n",
        "    print(stdout)\n",
        "    \n",
        "    # Verificar si nuestro modelo est√° en la lista\n",
        "    if model_name in stdout:\n",
        "        print(f\"\\n‚úÖ Tu modelo '{model_name}' est√° listo para usar\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Modelo '{model_name}' no encontrado en la lista\")\n",
        "else:\n",
        "    print(f\"‚ùå Error al listar modelos: {stderr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_model"
      },
      "source": [
        "## üß™ Probar el Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_chat"
      },
      "outputs": [],
      "source": [
        "def chat_with_model(prompt, model=model_name):\n",
        "    \"\"\"Funci√≥n para chatear con el modelo\"\"\"\n",
        "    print(f\"ü§ñ Pregunta: {prompt}\")\n",
        "    print(f\"üí≠ Respuesta de {model}:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Ejecutar ollama run\n",
        "    command = f'ollama run {model} \"{prompt}\"'\n",
        "    stdout, stderr, code = run_command(command)\n",
        "    \n",
        "    if code == 0:\n",
        "        print(stdout)\n",
        "    else:\n",
        "        print(f\"‚ùå Error: {stderr}\")\n",
        "    \n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# Pruebas del modelo\n",
        "test_prompts = [\n",
        "    \"¬øQu√© aprendiste durante tu fine-tuning en el Meta Day Uruguay 2025?\",\n",
        "    \"Explica qu√© es LoRA en t√©rminos simples\",\n",
        "    \"¬øCu√°les son las ventajas de usar Unsloth para fine-tuning?\",\n",
        "    \"¬øEs 9.11 mayor que 9.9? Explica tu razonamiento.\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Probando el modelo fine-tuneado...\\n\")\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nüîç Prueba {i}/{len(test_prompts)}\")\n",
        "    chat_with_model(prompt)\n",
        "\n",
        "print(\"\\n‚úÖ Todas las pruebas completadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive_chat"
      },
      "source": [
        "## üí¨ Chat Interactivo (Opcional)\n",
        "\n",
        "Para un chat m√°s interactivo, puedes usar la terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_instructions"
      },
      "outputs": [],
      "source": [
        "print(\"üí¨ Para iniciar un chat interactivo, ejecuta en tu terminal:\")\n",
        "print(f\"   ollama run {model_name}\")\n",
        "print(\"\\nüîß Comandos √∫tiles:\")\n",
        "print(f\"   ollama show {model_name}           # Ver informaci√≥n del modelo\")\n",
        "print(f\"   ollama rm {model_name}             # Eliminar el modelo\")\n",
        "print(f\"   ollama pull llama3.1               # Descargar modelo base\")\n",
        "print(f\"   ollama list                        # Listar todos los modelos\")\n",
        "\n",
        "print(\"\\nüåê Tambi√©n puedes usar la API REST de Ollama:\")\n",
        "print(\"   curl http://localhost:11434/api/generate -d '{\")\n",
        "print(f'     \"model\": \"{model_name}\",\")\n",
        "print('     \"prompt\": \"Tu pregunta aqu√≠\"')\n",
        "print(\"   }'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_example"
      },
      "source": [
        "## üîå Ejemplo de API REST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api_test"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def chat_via_api(prompt, model=model_name):\n",
        "    \"\"\"Usar la API REST de Ollama\"\"\"\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=data, timeout=30)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            return result.get(\"response\", \"Sin respuesta\")\n",
        "        else:\n",
        "            return f\"Error HTTP: {response.status_code}\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error de conexi√≥n: {e}\"\n",
        "\n",
        "# Probar API (solo si Ollama est√° ejecut√°ndose)\n",
        "print(\"üîå Probando API REST de Ollama...\")\n",
        "test_prompt = \"Hola, ¬øc√≥mo est√°s?\"\n",
        "response = chat_via_api(test_prompt)\n",
        "print(f\"Pregunta: {test_prompt}\")\n",
        "print(f\"Respuesta: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## üéØ Resumen y Pr√≥ximos Pasos\n",
        "\n",
        "### ‚úÖ Lo que hemos logrado:\n",
        "- Importado tu modelo fine-tuneado a Ollama\n",
        "- Configurado un Modelfile personalizado con ChatML template\n",
        "- Probado el modelo con diferentes preguntas\n",
        "- Aprendido a usar la API REST de Ollama\n",
        "\n",
        "### üöÄ Conexi√≥n con M√≥dulo 5 (RAG):\n",
        "Ahora puedes usar este modelo fine-tuneado en el **M√≥dulo 5** para:\n",
        "- Crear un sistema RAG personalizado\n",
        "- Combinar tu conocimiento fine-tuneado con documentos externos\n",
        "- Construir un chatbot especializado para tu dominio\n",
        "\n",
        "### üí° Ideas para expandir:\n",
        "1. **Integrar con LangChain**: Usar tu modelo en pipelines RAG\n",
        "2. **Crear una interfaz web**: Usar Streamlit o Gradio\n",
        "3. **Optimizar par√°metros**: Ajustar temperature, top_p seg√∫n tu uso\n",
        "4. **Monitorear rendimiento**: Comparar con modelo base\n",
        "5. **Escalar**: Probar con modelos m√°s grandes (70B)\n",
        "\n",
        "### üìö Recursos √∫tiles:\n",
        "- [Ollama Documentation](https://github.com/ollama/ollama) - Documentaci√≥n oficial\n",
        "- [Modelfile Reference](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) - Referencia completa\n",
        "- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md) - Documentaci√≥n de API\n",
        "\n",
        "---\n",
        "**Meta Day Uruguay 2025** - Puente M√≥dulo 4 ‚Üí M√≥dulo 5 üá∫üáæ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
