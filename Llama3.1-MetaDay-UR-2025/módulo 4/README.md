# M√≥dulo 4 ‚Äì Fine-tuning con QLoRA

## Contenido
- **üÜï Llama3.1_Unsloth_FineTuning_Optimizado.ipynb**: **RECOMENDADO** - Fine-tuning ultra-eficiente con Unsloth (2x m√°s r√°pido, 60% menos memoria)
- **üîß Solucion_Errores_Comunes.ipynb**: **EJECUTAR PRIMERO SI HAY ERRORES** - Soluciones para protobuf, CUDA, instalaci√≥n, etc.
- **üîó Usar_Modelo_FineTuneado_con_Ollama.ipynb**: C√≥mo usar tu modelo fine-tuneado con Ollama (conexi√≥n con M√≥dulo 5)
- Ejemplo_Qlora_cpu_with_outputs.ipynb: Fine-tuning QLoRA optimizado para CPU con salidas incluidas
- Ejemplo_Qlora_gpu_A100_no_outputs.ipynb: Fine-tuning QLoRA para GPU A100 (sin salidas pre-ejecutadas)

## Objetivos
- Aprender t√©cnicas de fine-tuning eficiente con QLoRA y **Unsloth**
- Implementar **Rank-Stabilized LoRA (rsLoRA)** para mejor estabilidad
- Usar datasets de ultra alta calidad (**FineTome-100k**)
- Optimizar hiperpar√°metros basados en mejores pr√°cticas
- Comparar rendimiento entre CPU y GPU
- Exportar modelos a m√∫ltiples formatos (LoRA, merged, GGUF)

## Requisitos

### Para CPU (Ejemplo_Qlora_cpu_with_outputs.ipynb)
- Python 3.10+
- Al menos 16GB RAM recomendado
- Dependencias:
  ```bash
  pip install torch transformers datasets peft bitsandbytes accelerate
  ```

### Para GPU (Ejemplo_Qlora_gpu_A100_no_outputs.ipynb)
- GPU con al menos 16GB VRAM (A100, V100, RTX 4090, etc.)
- CUDA 11.8+ o 12.x
- Dependencias:
  ```bash
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
  pip install transformers datasets peft bitsandbytes accelerate
  ```

## Ejecuci√≥n r√°pida

### üîß PASO 0: Si tienes errores (EJECUTAR PRIMERO)
1. Abre `Solucion_Errores_Comunes.ipynb`
2. **Errores cubiertos**: TypeError protobuf, CUDA out of memory, ImportError unsloth, device errors
3. **Soluciones autom√°ticas**: Instalaci√≥n limpia, configuraci√≥n preventiva, diagn√≥stico completo
4. **Tiempo**: 5-10 minutos para resolver la mayor√≠a de problemas

### üöÄ Opci√≥n RECOMENDADA: Unsloth Optimizado
1. Abre `Llama3.1_Unsloth_FineTuning_Optimizado.ipynb`
2. **Ventajas**: 2x m√°s r√°pido, 60% menos memoria, t√©cnicas state-of-the-art
3. Funciona en GPU T4/A100 y CPU (aunque m√°s lento)
4. Tiempo estimado: 20-30 min para 10k muestras en T4
5. Incluye exportaci√≥n a GGUF para Ollama/LM Studio
6. **Despu√©s del fine-tuning**: Ejecuta `Usar_Modelo_FineTuneado_con_Ollama.ipynb` para usar tu modelo localmente

### Opci√≥n 2: CPU (m√°s lento pero accesible)
1. Abre `Ejemplo_Qlora_cpu_with_outputs.ipynb`
2. Este notebook ya tiene salidas pre-ejecutadas para referencia
3. Puedes ejecutar celdas individuales o todo el notebook

### Opci√≥n 3: GPU A100 (m√©todo tradicional)
1. Verifica que tienes GPU disponible: `nvidia-smi`
2. Abre `Ejemplo_Qlora_gpu_A100_no_outputs.ipynb`
3. Ejecuta las celdas en orden
4. Tiempo estimado: 30-60 minutos dependiendo del dataset

## Alternativas en la nube
- Google Colab Pro (GPU T4/A100)
- Kaggle Notebooks (GPU P100/T4)
- AWS SageMaker, Azure ML, GCP Vertex AI

## Notas importantes
- **Unsloth** ofrece las mejores optimizaciones disponibles actualmente
- **rsLoRA** mejora la estabilidad del entrenamiento con ranks altos
- **FineTome-100k** es un dataset de ultra alta calidad filtrado con clasificadores educativos
- QLoRA reduce significativamente el uso de memoria comparado con fine-tuning completo
- Los ejemplos usan modelos peque√±os para demostraci√≥n; en producci√≥n usa modelos m√°s grandes
- Ajusta `max_steps`, `batch_size` seg√∫n tu hardware disponible

## Nuevas caracter√≠sticas implementadas
- ‚úÖ **Unsloth**: 2x m√°s r√°pido, 60% menos memoria
- ‚úÖ **rsLoRA**: Rank-Stabilized LoRA para mejor estabilidad
- ‚úÖ **Chat templates**: ChatML optimizado para conversaciones
- ‚úÖ **Dataset de calidad**: FineTome-100k filtrado
- ‚úÖ **Hiperpar√°metros optimizados**: Basados en investigaci√≥n reciente
- ‚úÖ **Exportaci√≥n m√∫ltiple**: LoRA, merged, GGUF
- ‚úÖ **AdamW 8-bit**: Optimizador eficiente en memoria
