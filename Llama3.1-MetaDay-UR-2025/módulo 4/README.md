# M√≥dulo 4 ‚Äì Fine-tuning con QLoRA

## Contenido
- **üÜï Llama3.1_Unsloth_FineTuning_Optimizado.ipynb**: **RECOMENDADO** - Fine-tuning ultra-eficiente con Unsloth (2x m√°s r√°pido, 60% menos memoria)
- **üîó RAG_Modelo_Optimizado_Unsloth.ipynb**: **RAG SIN OLLAMA** - Sistema RAG completo usando directamente el modelo optimizado
- **üéØ Evaluacion_Modelo_Optimizado_Simple.ipynb**: **SIN CONFLICTOS** - Evaluaci√≥n directa sin instalaciones, evita errores PyTorch/torchaudio
- **‚ö° Evaluacion_Optimizada_Modelo_HF.ipynb**: **EVALUACI√ìN CON TU C√ìDIGO OPTIMIZADO** - Usa FastLanguageModel.for_inference() y chat templates optimizados
- **üß™ Probar_Modelo_FineTuneado_HuggingFace.ipynb**: **EVALUACI√ìN COMPLETA** - Tests comprehensivos del modelo `alvarezpablo/llama3.1-8b-finetune-metaday`
- **üîß Solucion_Errores_Comunes.ipynb**: **EJECUTAR PRIMERO SI HAY ERRORES** - Soluciones para protobuf, CUDA, instalaci√≥n, etc.
- **üîó Usar_Modelo_FineTuneado_con_Ollama.ipynb**: C√≥mo usar tu modelo fine-tuneado con Ollama (conexi√≥n con M√≥dulo 5)
- Ejemplo_Qlora_cpu_with_outputs.ipynb: Fine-tuning QLoRA optimizado para CPU con salidas incluidas
- Ejemplo_Qlora_gpu_A100_no_outputs.ipynb: Fine-tuning QLoRA para GPU A100 (sin salidas pre-ejecutadas)

## Objetivos
- Aprender t√©cnicas de fine-tuning eficiente con QLoRA y **Unsloth**
- Implementar **Rank-Stabilized LoRA (rsLoRA)** para mejor estabilidad
- Usar datasets de ultra alta calidad (**FineTome-100k**)
- Optimizar hiperpar√°metros basados en mejores pr√°cticas
- Comparar rendimiento entre CPU y GPU
- Exportar modelos a m√∫ltiples formatos (LoRA, merged, GGUF)

## Requisitos

### Para CPU (Ejemplo_Qlora_cpu_with_outputs.ipynb)
- Python 3.10+
- Al menos 16GB RAM recomendado
- Dependencias:
  ```bash
  pip install torch transformers datasets peft bitsandbytes accelerate
  ```

### Para GPU (Ejemplo_Qlora_gpu_A100_no_outputs.ipynb)
- GPU con al menos 16GB VRAM (A100, V100, RTX 4090, etc.)
- CUDA 11.8+ o 12.x
- Dependencias:
  ```bash
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
  pip install transformers datasets peft bitsandbytes accelerate
  ```

## Ejecuci√≥n r√°pida

### üîß PASO 0: Si tienes errores (EJECUTAR PRIMERO)
1. Abre `Solucion_Errores_Comunes.ipynb`
2. **Errores cubiertos**: TypeError protobuf, CUDA out of memory, ImportError unsloth, device errors
3. **Soluciones autom√°ticas**: Instalaci√≥n limpia, configuraci√≥n preventiva, diagn√≥stico completo
4. **Tiempo**: 5-10 minutos para resolver la mayor√≠a de problemas

### üéØ OPCI√ìN RECOMENDADA: Sin Conflictos de Dependencias
1. Abre `Evaluacion_Modelo_Optimizado_Simple.ipynb`
2. **Sin instalaciones**: Usa dependencias existentes, evita conflictos PyTorch/torchaudio/fastai
3. **Tu c√≥digo optimizado**: FastLanguageModel + chat templates + use_cache=True
4. **Modelo pre-optimizado**: Ya tiene optimizaciones Unsloth del fine-tuning
5. **Tests focalizados**: Verificaci√≥n r√°pida + tests extendidos opcionales
6. **Tiempo estimado**: 5-10 min para verificaci√≥n, 15-20 min completo

### üîó OPCI√ìN RAG OPTIMIZADO: Sin Ollama
1. Abre `RAG_Modelo_Optimizado_Unsloth.ipynb`
2. **Sistema RAG completo**: Retrieval + Generation con tu modelo optimizado
3. **Sin Ollama**: Usa directamente el modelo Unsloth + HuggingFace embeddings
4. **Ventajas**: 2-3x m√°s r√°pido, control total, sin dependencias externas
5. **Dataset incluido**: Chistes en espa√±ol para demostraci√≥n
6. **Tiempo estimado**: 20-30 min para setup completo + tests

### üöÄ Opci√≥n FINE-TUNING: Unsloth Optimizado
1. Abre `Llama3.1_Unsloth_FineTuning_Optimizado.ipynb`
2. **Ventajas**: 2x m√°s r√°pido, 60% menos memoria, t√©cnicas state-of-the-art
3. Funciona en GPU T4/A100 y CPU (aunque m√°s lento)
4. Tiempo estimado: 20-30 min para 10k muestras en T4
5. Incluye exportaci√≥n a GGUF para Ollama/LM Studio
6. **Despu√©s del fine-tuning**: Ejecuta `Usar_Modelo_FineTuneado_con_Ollama.ipynb` para usar tu modelo localmente

### ‚ö° Opci√≥n EVALUACI√ìN OPTIMIZADA: Tu C√≥digo Mejorado
1. Abre `Evaluacion_Optimizada_Modelo_HF.ipynb`
2. **Usa tu c√≥digo optimizado**: FastLanguageModel.for_inference(), chat templates mejorados
3. **Velocidad 2x m√°s r√°pida**: use_cache=True, decodificaci√≥n eficiente
4. **Tests focalizados**: 20 tests en 5 categor√≠as principales
5. **Visualizaci√≥n en tiempo real**: TextStreamer para ver respuestas gener√°ndose
6. **Tiempo estimado**: 10-15 min para evaluaci√≥n completa

### üß™ Opci√≥n EVALUACI√ìN COMPLETA: Probar Modelo desde Hugging Face
1. Abre `Probar_Modelo_FineTuneado_HuggingFace.ipynb`
2. **Modelo pre-cargado**: `alvarezpablo/llama3.1-8b-finetune-metaday`
3. **8 suites de tests**: Matem√°ticas, conversaci√≥n, programaci√≥n, conocimiento, idiomas, creatividad, an√°lisis, casos edge
4. **56 tests √∫nicos** con m√©tricas autom√°ticas de calidad
5. **Exportaci√≥n completa**: JSON, CSV, y reporte markdown
6. **Tiempo estimado**: 15-25 min para evaluaci√≥n completa

### Opci√≥n 3: CPU (m√°s lento pero accesible)
1. Abre `Ejemplo_Qlora_cpu_with_outputs.ipynb`
2. Este notebook ya tiene salidas pre-ejecutadas para referencia
3. Puedes ejecutar celdas individuales o todo el notebook

### Opci√≥n 4: GPU A100 (m√©todo tradicional)
1. Verifica que tienes GPU disponible: `nvidia-smi`
2. Abre `Ejemplo_Qlora_gpu_A100_no_outputs.ipynb`
3. Ejecuta las celdas en orden
4. Tiempo estimado: 30-60 minutos dependiendo del dataset

## Alternativas en la nube
- Google Colab Pro (GPU T4/A100)
- Kaggle Notebooks (GPU P100/T4)
- AWS SageMaker, Azure ML, GCP Vertex AI

## Notas importantes
- **Unsloth** ofrece las mejores optimizaciones disponibles actualmente
- **rsLoRA** mejora la estabilidad del entrenamiento con ranks altos
- **FineTome-100k** es un dataset de ultra alta calidad filtrado con clasificadores educativos
- QLoRA reduce significativamente el uso de memoria comparado con fine-tuning completo
- Los ejemplos usan modelos peque√±os para demostraci√≥n; en producci√≥n usa modelos m√°s grandes
- Ajusta `max_steps`, `batch_size` seg√∫n tu hardware disponible

## Nuevas caracter√≠sticas implementadas
- ‚úÖ **Unsloth**: 2x m√°s r√°pido, 60% menos memoria
- ‚úÖ **rsLoRA**: Rank-Stabilized LoRA para mejor estabilidad
- ‚úÖ **Chat templates**: ChatML optimizado para conversaciones
- ‚úÖ **Dataset de calidad**: FineTome-100k filtrado
- ‚úÖ **Hiperpar√°metros optimizados**: Basados en investigaci√≥n reciente
- ‚úÖ **Exportaci√≥n m√∫ltiple**: LoRA, merged, GGUF
- ‚úÖ **AdamW 8-bit**: Optimizador eficiente en memoria
- ‚úÖ **Evaluaci√≥n comprehensiva**: 56 tests en 8 categor√≠as diferentes
- ‚úÖ **M√©tricas autom√°ticas**: Velocidad, calidad, completitud, estructura
- ‚úÖ **C√≥digo optimizado validado**: FastLanguageModel.for_inference() + chat templates
- ‚úÖ **Evaluaci√≥n en tiempo real**: TextStreamer para visualizaci√≥n inmediata
- ‚úÖ **RAG optimizado**: Sistema completo sin Ollama, 2-3x m√°s r√°pido
- ‚úÖ **Embeddings locales**: HuggingFace multiling√ºe en lugar de Ollama
