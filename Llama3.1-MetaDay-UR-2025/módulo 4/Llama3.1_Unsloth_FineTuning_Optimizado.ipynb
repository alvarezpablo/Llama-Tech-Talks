{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvarezpablo/Llama-Tech-Talks/blob/main/Llama3.1-MetaDay-UR-2025/m√≥dulo%204/Llama3.1_Unsloth_FineTuning_Optimizado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ü¶ô Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth\n",
        "\n",
        "## Meta Day Uruguay 2025 - M√≥dulo 4 Optimizado\n",
        "\n",
        "Este notebook implementa las t√©cnicas m√°s avanzadas de fine-tuning usando **Unsloth** para obtener:\n",
        "- ‚ö° **2x m√°s r√°pido** que m√©todos tradicionales\n",
        "- üíæ **60% menos uso de memoria**\n",
        "- üéØ **Rank-Stabilized LoRA (rsLoRA)**\n",
        "- üìä **Chat templates optimizados**\n",
        "- üîß **Configuraci√≥n autom√°tica de hiperpar√°metros**\n",
        "\n",
        "Basado en el art√≠culo: [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üöÄ Configuraci√≥n e Instalaci√≥n\n",
        "\n",
        "### Verificar GPU disponible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu_check"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Ejecutando en CPU - El entrenamiento ser√° m√°s lento\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "### Instalar Unsloth y dependencias optimizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_unsloth"
      },
      "outputs": [],
      "source": [
        "# üîß Soluci√≥n para error de protobuf\n",
        "import os\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "\n",
        "# Instalaci√≥n optimizada para Colab\n",
        "!pip install \"protobuf<=3.20.3\"\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "print(\"‚úÖ Instalaci√≥n completada con fix de protobuf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "### Importar librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"üöÄ Configuraci√≥n completada - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_loading"
      },
      "source": [
        "## ü§ñ Carga del Modelo Llama 3.1 8B\n",
        "\n",
        "Usamos la versi√≥n pre-cuantizada de Unsloth para m√°xima eficiencia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Configuraci√≥n del modelo\n",
        "max_seq_length = 2048  # Ajusta seg√∫n tu GPU (hasta 128k para Llama 3.1)\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
        "\n",
        "print(f\"üì• Cargando modelo: {model_name}\")\n",
        "print(f\"üìè Longitud m√°xima de secuencia: {max_seq_length}\")\n",
        "\n",
        "# Cargar modelo y tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,  # Auto-detecta BF16 para GPUs Ampere+\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo cargado exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_config"
      },
      "source": [
        "## ‚öôÔ∏è Configuraci√≥n LoRA Optimizada\n",
        "\n",
        "Implementamos **Rank-Stabilized LoRA (rsLoRA)** con configuraci√≥n optimizada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_lora"
      },
      "outputs": [],
      "source": [
        "# Configuraci√≥n LoRA optimizada\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Rank - balance entre calidad y eficiencia\n",
        "    lora_alpha=16,  # Scaling factor (t√≠picamente 1x o 2x el rank)\n",
        "    lora_dropout=0,  # Sin dropout para entrenamiento m√°s r√°pido\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\",  # Attention\n",
        "        \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"  # Feed-forward\n",
        "    ], \n",
        "    use_rslora=True,  # üéØ Rank-Stabilized LoRA para mejor estabilidad\n",
        "    use_gradient_checkpointing=\"unsloth\"  # Optimizaci√≥n de memoria\n",
        ")\n",
        "\n",
        "# Mostrar estad√≠sticas de par√°metros\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print(f\"üìä Par√°metros totales: {total_params:,}\")\n",
        "print(f\"üéØ Par√°metros entrenables: {trainable_params:,} ({percentage:.4f}%)\")\n",
        "print(f\"üíæ Reducci√≥n de par√°metros: {100-percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_section"
      },
      "source": [
        "## üìö Preparaci√≥n del Dataset\n",
        "\n",
        "Usamos el dataset **FineTome-100k** - ultra alta calidad con conversaciones, razonamiento y function calling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_chat_template"
      },
      "outputs": [],
      "source": [
        "# Configurar chat template (ChatML - est√°ndar de la comunidad)\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
        "    chat_template=\"chatml\",\n",
        ")\n",
        "\n",
        "def apply_template(examples):\n",
        "    \"\"\"Aplica el chat template a las conversaciones\"\"\"\n",
        "    messages = examples[\"conversations\"]\n",
        "    text = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            message, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=False\n",
        "        ) for message in messages\n",
        "    ]\n",
        "    return {\"text\": text}\n",
        "\n",
        "print(\"‚úÖ Chat template configurado (ChatML)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "source": [
        "# Cargar dataset - ajusta el subset para entrenamientos m√°s r√°pidos\n",
        "dataset_size = \"train[:10000]\"  # Cambia a \"train\" para dataset completo (100k samples)\n",
        "\n",
        "print(f\"üì• Cargando dataset: mlabonne/FineTome-100k ({dataset_size})\")\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=dataset_size)\n",
        "dataset = dataset.map(apply_template, batched=True)\n",
        "\n",
        "print(f\"üìä Dataset cargado: {len(dataset):,} muestras\")\n",
        "print(f\"üìù Ejemplo de conversaci√≥n formateada:\")\n",
        "print(\"=\" * 50)\n",
        "print(dataset[0][\"text\"][:500] + \"...\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section"
      },
      "source": [
        "## üèãÔ∏è Entrenamiento con Hiperpar√°metros Optimizados\n",
        "\n",
        "Configuraci√≥n basada en las mejores pr√°cticas del art√≠culo de Hugging Face:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_config"
      },
      "outputs": [],
      "source": [
        "# Configuraci√≥n de entrenamiento optimizada\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,  # üöÄ Combina m√∫ltiples muestras peque√±as en un batch\n",
        "    args=TrainingArguments(\n",
        "        # Configuraci√≥n de learning rate\n",
        "        learning_rate=3e-4,  # √ìptimo para LoRA\n",
        "        lr_scheduler_type=\"linear\",  # Scheduler lineal recomendado\n",
        "        \n",
        "        # Configuraci√≥n de batch\n",
        "        per_device_train_batch_size=8,  # Ajusta seg√∫n tu GPU\n",
        "        gradient_accumulation_steps=2,  # Batch efectivo = 8 * 2 = 16\n",
        "        \n",
        "        # √âpocas y pasos\n",
        "        num_train_epochs=1,  # 1 √©poca suele ser suficiente con datasets de calidad\n",
        "        warmup_steps=10,  # Warmup para estabilizar entrenamiento inicial\n",
        "        \n",
        "        # Optimizaci√≥n\n",
        "        fp16=not is_bfloat16_supported(),  # FP16 para GPUs m√°s antiguas\n",
        "        bf16=is_bfloat16_supported(),      # BF16 para GPUs Ampere+\n",
        "        optim=\"adamw_8bit\",  # üéØ AdamW 8-bit para menor uso de memoria\n",
        "        weight_decay=0.01,   # Regularizaci√≥n\n",
        "        \n",
        "        # Logging y guardado\n",
        "        logging_steps=1,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,  # Reproducibilidad\n",
        "        \n",
        "        # Configuraciones adicionales\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_pin_memory=False,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"‚öôÔ∏è Trainer configurado con hiperpar√°metros optimizados\")\n",
        "print(f\"üéØ Batch size efectivo: {8 * 2} (per_device_batch_size * gradient_accumulation_steps)\")\n",
        "print(f\"üîß Optimizador: AdamW 8-bit\")\n",
        "print(f\"üìä Precisi√≥n: {'BF16' if is_bfloat16_supported() else 'FP16'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_training"
      },
      "outputs": [],
      "source": [
        "# üöÄ ¬°Iniciar entrenamiento!\n",
        "print(\"üèãÔ∏è Iniciando entrenamiento...\")\n",
        "print(f\"‚è∞ Tiempo estimado: ~20-30 min para 10k muestras en T4\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üéâ ¬°Entrenamiento completado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference_section"
      },
      "source": [
        "## üß™ Prueba del Modelo Fine-tuneado\n",
        "\n",
        "Probemos el modelo con algunas preguntas para verificar su funcionamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model"
      },
      "outputs": [],
      "source": [
        "# Preparar modelo para inferencia (2x m√°s r√°pido)\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "def test_model(prompt, max_tokens=128):\n",
        "    \"\"\"Funci√≥n helper para probar el modelo\"\"\"\n",
        "    messages = [{\"from\": \"human\", \"value\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    text_streamer = TextStreamer(tokenizer)\n",
        "    print(f\"ü§ñ Pregunta: {prompt}\")\n",
        "    print(f\"üí≠ Respuesta: \", end=\"\")\n",
        "    \n",
        "    _ = model.generate(\n",
        "        input_ids=inputs, \n",
        "        streamer=text_streamer, \n",
        "        max_new_tokens=max_tokens, \n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"üß™ Probando el modelo fine-tuneado...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_tests"
      },
      "outputs": [],
      "source": [
        "# Pruebas del modelo\n",
        "test_prompts = [\n",
        "    \"¬øEs 9.11 mayor que 9.9?\",\n",
        "    \"Explica qu√© es el fine-tuning en t√©rminos simples\",\n",
        "    \"¬øCu√°les son las ventajas de usar LoRA?\",\n",
        "    \"Escribe un c√≥digo Python para calcular la secuencia de Fibonacci\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nüîç Prueba {i}/{len(test_prompts)}\")\n",
        "    test_model(prompt)\n",
        "    \n",
        "print(\"\\n‚úÖ Todas las pruebas completadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_section"
      },
      "source": [
        "## üíæ Guardar y Exportar el Modelo\n",
        "\n",
        "Guardamos el modelo en diferentes formatos para m√°xima compatibilidad:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model"
      },
      "outputs": [],
      "source": [
        "# Opci√≥n 1: Guardar solo los adaptadores LoRA (m√°s peque√±o)\n",
        "print(\"üíæ Guardando adaptadores LoRA...\")\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "print(\"‚úÖ Adaptadores LoRA guardados en './lora_model'\")\n",
        "\n",
        "# Opci√≥n 2: Guardar modelo completo fusionado (16-bit)\n",
        "print(\"\\nüîó Fusionando y guardando modelo completo...\")\n",
        "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method=\"merged_16bit\")\n",
        "print(\"‚úÖ Modelo fusionado guardado en './merged_model'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_hf"
      },
      "outputs": [],
      "source": [
        "# Opcional: Subir a Hugging Face Hub\n",
        "# Descomenta y configura tu token de HF para subir el modelo\n",
        "\n",
        "# from huggingface_hub import login\n",
        "# login()  # Ingresa tu token de Hugging Face\n",
        "\n",
        "# # Subir modelo fusionado\n",
        "# model_name = \"tu-usuario/llama3.1-8b-finetune-metaday\"\n",
        "# model.push_to_hub_merged(model_name, tokenizer, save_method=\"merged_16bit\")\n",
        "# print(f\"üöÄ Modelo subido a: https://huggingface.co/{model_name}\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è Para subir a HF Hub, descomenta y configura el c√≥digo anterior\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gguf_section"
      },
      "source": [
        "### üì¶ Exportar a formato GGUF (para Ollama, LM Studio, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_gguf"
      },
      "outputs": [],
      "source": [
        "# Exportar a GGUF para usar con Ollama, LM Studio, etc.\n",
        "print(\"üì¶ Exportando a formato GGUF...\")\n",
        "\n",
        "# Diferentes niveles de cuantizaci√≥n\n",
        "quant_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]  # M√©todos m√°s comunes\n",
        "\n",
        "for quant in quant_methods:\n",
        "    print(f\"üîÑ Exportando {quant}...\")\n",
        "    model.save_pretrained_gguf(f\"gguf_model\", tokenizer, quantization_method=quant)\n",
        "    print(f\"‚úÖ {quant} guardado\")\n",
        "\n",
        "print(\"\\nüéâ Todos los formatos GGUF exportados en './gguf_model'\")\n",
        "print(\"üí° Puedes usar estos archivos con:\")\n",
        "print(\"   ‚Ä¢ Ollama: ollama create mi-modelo -f ./gguf_model\")\n",
        "print(\"   ‚Ä¢ LM Studio: Importar directamente\")\n",
        "print(\"   ‚Ä¢ llama.cpp: ./main -m ./gguf_model/model-q4_k_m.gguf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## üéØ Resumen y Pr√≥ximos Pasos\n",
        "\n",
        "### ‚úÖ Lo que hemos logrado:\n",
        "- Fine-tuning ultra-eficiente con **Unsloth** (2x m√°s r√°pido, 60% menos memoria)\n",
        "- Implementaci√≥n de **Rank-Stabilized LoRA (rsLoRA)** para mejor estabilidad\n",
        "- Uso del dataset **FineTome-100k** de ultra alta calidad\n",
        "- Chat template **ChatML** optimizado\n",
        "- Hiperpar√°metros basados en mejores pr√°cticas\n",
        "- Exportaci√≥n a m√∫ltiples formatos (LoRA, merged, GGUF)\n",
        "\n",
        "### üöÄ Pr√≥ximos pasos sugeridos:\n",
        "1. **Evaluaci√≥n**: Usar Open LLM Leaderboard o LLM AutoEval\n",
        "2. **Alignment**: Aplicar DPO con dataset de preferencias\n",
        "3. **Cuantizaci√≥n**: Probar EXL2, AWQ, GPTQ para inferencia m√°s r√°pida\n",
        "4. **Deployment**: Usar Hugging Face Spaces, Ollama, o vLLM\n",
        "5. **Escalado**: Probar con modelos m√°s grandes (70B, 405B)\n",
        "\n",
        "### üìö Recursos adicionales:\n",
        "- [LLM Course](https://github.com/mlabonne/llm-course) - Curso completo de LLMs\n",
        "- [Unsloth Documentation](https://github.com/unslothai/unsloth) - Documentaci√≥n oficial\n",
        "- [FineTome Dataset](https://huggingface.co/datasets/mlabonne/FineTome-100k) - Dataset usado\n",
        "\n",
        "---\n",
        "**Meta Day Uruguay 2025** - M√≥dulo 4: Fine-tuning Optimizado üá∫üáæ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
