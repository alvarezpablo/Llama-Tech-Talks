{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# üîß Soluci√≥n de Errores Comunes - Fine-tuning\n",
        "\n",
        "## Meta Day Uruguay 2025 - M√≥dulo 4: Troubleshooting\n",
        "\n",
        "Este notebook contiene soluciones para los errores m√°s comunes al hacer fine-tuning con Unsloth y QLoRA.\n",
        "\n",
        "### üö® Errores cubiertos:\n",
        "1. **TypeError: Descriptors cannot be created directly** (protobuf)\n",
        "2. **CUDA out of memory**\n",
        "3. **ImportError: No module named 'unsloth'**\n",
        "4. **RuntimeError: Expected all tensors to be on the same device**\n",
        "5. **Problemas de versiones de dependencias**\n",
        "6. **Errores de tokenizaci√≥n**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "protobuf_error"
      },
      "source": [
        "## üî¥ Error 1: TypeError - Descriptors cannot be created directly\n",
        "\n",
        "**Error completo:**\n",
        "```\n",
        "TypeError: Descriptors cannot be created directly.\n",
        "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
        "```\n",
        "\n",
        "**Causa:** Incompatibilidad entre versiones de protobuf\n",
        "\n",
        "**Soluci√≥n:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fix_protobuf"
      },
      "outputs": [],
      "source": [
        "# üîß SOLUCI√ìN COMPLETA PARA ERROR DE PROTOBUF\n",
        "\n",
        "# M√©todo 1: Variable de entorno (m√°s r√°pido pero menos eficiente)\n",
        "import os\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "print(\"‚úÖ Variable de entorno configurada\")\n",
        "\n",
        "# M√©todo 2: Downgrade de protobuf (recomendado)\n",
        "print(\"üîÑ Instalando versi√≥n compatible de protobuf...\")\n",
        "%pip install --upgrade \"protobuf<=3.20.3\" --quiet\n",
        "\n",
        "# Verificar versi√≥n\n",
        "import protobuf\n",
        "print(f\"üì¶ Protobuf version: {protobuf.__version__}\")\n",
        "\n",
        "# Reiniciar runtime si es necesario\n",
        "print(\"‚ö†Ô∏è Si el error persiste, reinicia el runtime: Runtime > Restart Runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuda_memory"
      },
      "source": [
        "## üî¥ Error 2: CUDA out of memory\n",
        "\n",
        "**Error:** `RuntimeError: CUDA out of memory`\n",
        "\n",
        "**Soluciones:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fix_cuda_memory"
      },
      "outputs": [],
      "source": [
        "# üîß SOLUCIONES PARA MEMORIA GPU INSUFICIENTE\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Limpiar memoria GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"üßπ Memoria GPU limpiada\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No hay GPU disponible\")\n",
        "\n",
        "def check_gpu_memory():\n",
        "    \"\"\"Verificar uso de memoria GPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "        cached = torch.cuda.memory_reserved(0) / 1024**3\n",
        "        free = total - cached\n",
        "        \n",
        "        print(f\"üìä Memoria GPU:\")\n",
        "        print(f\"   Total: {total:.1f} GB\")\n",
        "        print(f\"   Asignada: {allocated:.1f} GB\")\n",
        "        print(f\"   En cach√©: {cached:.1f} GB\")\n",
        "        print(f\"   Libre: {free:.1f} GB\")\n",
        "        \n",
        "        if free < 2.0:\n",
        "            print(\"‚ö†Ô∏è Poca memoria libre - considera reducir batch_size\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No hay GPU disponible\")\n",
        "\n",
        "# Ejecutar diagn√≥stico\n",
        "clear_gpu_memory()\n",
        "check_gpu_memory()\n",
        "\n",
        "print(\"\\nüí° Soluciones si tienes poco memoria:\")\n",
        "print(\"1. Reducir max_seq_length (ej: 1024 en lugar de 2048)\")\n",
        "print(\"2. Reducir per_device_train_batch_size (ej: 2 en lugar de 8)\")\n",
        "print(\"3. Aumentar gradient_accumulation_steps para compensar\")\n",
        "print(\"4. Usar modelo m√°s peque√±o (7B en lugar de 8B)\")\n",
        "print(\"5. Activar gradient_checkpointing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unsloth_import"
      },
      "source": [
        "## üî¥ Error 3: ImportError - No module named 'unsloth'\n",
        "\n",
        "**Soluci√≥n de instalaci√≥n paso a paso:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fix_unsloth_install"
      },
      "outputs": [],
      "source": [
        "# üîß INSTALACI√ìN PASO A PASO DE UNSLOTH\n",
        "\n",
        "print(\"üîÑ Instalando Unsloth y dependencias...\")\n",
        "\n",
        "# Paso 1: Limpiar instalaciones previas\n",
        "%pip uninstall -y unsloth unsloth-zoo --quiet\n",
        "\n",
        "# Paso 2: Instalar dependencias base\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
        "%pip install transformers datasets accelerate peft --quiet\n",
        "\n",
        "# Paso 3: Instalar bitsandbytes\n",
        "%pip install bitsandbytes --quiet\n",
        "\n",
        "# Paso 4: Instalar Unsloth\n",
        "# Para Colab:\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
        "\n",
        "# Para otros entornos, usar:\n",
        "# %pip install unsloth --quiet\n",
        "\n",
        "# Paso 5: Verificar instalaci√≥n\n",
        "try:\n",
        "    from unsloth import FastLanguageModel\n",
        "    print(\"‚úÖ Unsloth instalado correctamente\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error al importar Unsloth: {e}\")\n",
        "    print(\"üí° Intenta reiniciar el runtime y ejecutar de nuevo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "device_error"
      },
      "source": [
        "## üî¥ Error 4: Expected all tensors to be on the same device\n",
        "\n",
        "**Soluci√≥n:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fix_device_error"
      },
      "outputs": [],
      "source": [
        "# üîß SOLUCI√ìN PARA ERRORES DE DISPOSITIVO\n",
        "\n",
        "import torch\n",
        "\n",
        "def check_device_setup():\n",
        "    \"\"\"Verificar configuraci√≥n de dispositivos\"\"\"\n",
        "    print(\"üîç Verificando configuraci√≥n de dispositivos...\")\n",
        "    \n",
        "    # Verificar CUDA\n",
        "    cuda_available = torch.cuda.is_available()\n",
        "    print(f\"CUDA disponible: {cuda_available}\")\n",
        "    \n",
        "    if cuda_available:\n",
        "        device_count = torch.cuda.device_count()\n",
        "        current_device = torch.cuda.current_device()\n",
        "        device_name = torch.cuda.get_device_name(current_device)\n",
        "        \n",
        "        print(f\"Dispositivos GPU: {device_count}\")\n",
        "        print(f\"GPU actual: {current_device} ({device_name})\")\n",
        "        \n",
        "        # Configurar dispositivo por defecto\n",
        "        torch.cuda.set_device(0)\n",
        "        print(\"‚úÖ Dispositivo GPU configurado\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Usando CPU - el entrenamiento ser√° m√°s lento\")\n",
        "\n",
        "def fix_model_device(model, tokenizer):\n",
        "    \"\"\"Asegurar que modelo y tokenizer est√©n en el dispositivo correcto\"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # Mover modelo al dispositivo\n",
        "    if hasattr(model, 'to'):\n",
        "        model = model.to(device)\n",
        "        print(f\"‚úÖ Modelo movido a {device}\")\n",
        "    \n",
        "    # Configurar tokenizer\n",
        "    if hasattr(tokenizer, 'pad_token') and tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(\"‚úÖ Pad token configurado\")\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "# Ejecutar verificaci√≥n\n",
        "check_device_setup()\n",
        "\n",
        "print(\"\\nüí° Si el error persiste:\")\n",
        "print(\"1. Reinicia el runtime\")\n",
        "print(\"2. Aseg√∫rate de usar FastLanguageModel.from_pretrained()\")\n",
        "print(\"3. No mezcles operaciones CPU/GPU manualmente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "version_conflicts"
      },
      "source": [
        "## üî¥ Error 5: Conflictos de Versiones\n",
        "\n",
        "**Instalaci√≥n limpia con versiones compatibles:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fix_versions"
      },
      "outputs": [],
      "source": [
        "# üîß INSTALACI√ìN LIMPIA CON VERSIONES COMPATIBLES\n",
        "\n",
        "print(\"üßπ Limpiando instalaciones previas...\")\n",
        "\n",
        "# Desinstalar paquetes problem√°ticos\n",
        "packages_to_remove = [\n",
        "    \"unsloth\", \"unsloth-zoo\", \"xformers\", \"trl\", \n",
        "    \"transformers\", \"accelerate\", \"peft\", \"bitsandbytes\"\n",
        "]\n",
        "\n",
        "for package in packages_to_remove:\n",
        "    %pip uninstall -y {package} --quiet\n",
        "\n",
        "print(\"üì¶ Instalando versiones compatibles...\")\n",
        "\n",
        "# Instalar versiones espec√≠ficas compatibles\n",
        "compatible_packages = {\n",
        "    \"protobuf\": \"<=3.20.3\",\n",
        "    \"transformers\": \">=4.36.0\",\n",
        "    \"accelerate\": \">=0.21.0\",\n",
        "    \"peft\": \">=0.4.0\",\n",
        "    \"bitsandbytes\": \">=0.41.0\",\n",
        "    \"trl\": \"<0.9.0\",\n",
        "    \"xformers\": \"<0.0.27\"\n",
        "}\n",
        "\n",
        "for package, version in compatible_packages.items():\n",
        "    print(f\"Installing {package}{version}...\")\n",
        "    %pip install \"{package}{version}\" --quiet\n",
        "\n",
        "# Instalar Unsloth al final\n",
        "print(\"Installing Unsloth...\")\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
        "\n",
        "print(\"\\n‚úÖ Instalaci√≥n limpia completada\")\n",
        "print(\"‚ö†Ô∏è IMPORTANTE: Reinicia el runtime antes de continuar\")\n",
        "print(\"   Runtime > Restart Runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenization_error"
      },
      "source": [
        "## üî¥ Error 6: Errores de Tokenizaci√≥n\n",
        "\n",
        "**Problemas comunes con tokens y chat templates:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fix_tokenization"
      },
      "outputs": [],
      "source": [
        "# üîß SOLUCI√ìN PARA ERRORES DE TOKENIZACI√ìN\n",
        "\n",
        "def fix_tokenizer_issues(tokenizer):\n",
        "    \"\"\"Solucionar problemas comunes de tokenizaci√≥n\"\"\"\n",
        "    print(\"üîß Configurando tokenizer...\")\n",
        "    \n",
        "    # Configurar pad token si no existe\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(\"‚úÖ Pad token configurado\")\n",
        "    \n",
        "    # Configurar padding side\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    print(\"‚úÖ Padding side configurado\")\n",
        "    \n",
        "    # Verificar tokens especiales\n",
        "    special_tokens = {\n",
        "        \"bos_token\": tokenizer.bos_token,\n",
        "        \"eos_token\": tokenizer.eos_token,\n",
        "        \"pad_token\": tokenizer.pad_token,\n",
        "        \"unk_token\": tokenizer.unk_token\n",
        "    }\n",
        "    \n",
        "    print(\"üìã Tokens especiales:\")\n",
        "    for name, token in special_tokens.items():\n",
        "        print(f\"   {name}: {token}\")\n",
        "    \n",
        "    return tokenizer\n",
        "\n",
        "def test_chat_template(tokenizer):\n",
        "    \"\"\"Probar chat template\"\"\"\n",
        "    print(\"üß™ Probando chat template...\")\n",
        "    \n",
        "    # Mensaje de prueba\n",
        "    messages = [\n",
        "        {\"from\": \"human\", \"value\": \"Hola, ¬øc√≥mo est√°s?\"},\n",
        "        {\"from\": \"gpt\", \"value\": \"¬°Hola! Estoy bien, gracias por preguntar.\"}\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        # Aplicar template\n",
        "        formatted = tokenizer.apply_chat_template(\n",
        "            messages, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        print(\"‚úÖ Chat template funcionando\")\n",
        "        print(\"üìù Ejemplo formateado:\")\n",
        "        print(formatted[:200] + \"...\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en chat template: {e}\")\n",
        "        print(\"üí° Soluci√≥n: Configurar template manualmente\")\n",
        "\n",
        "# Ejemplo de uso (descomenta cuando tengas un tokenizer)\n",
        "# tokenizer = fix_tokenizer_issues(tokenizer)\n",
        "# test_chat_template(tokenizer)\n",
        "\n",
        "print(\"üí° Ejecuta estas funciones despu√©s de cargar tu tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_fixes"
      },
      "source": [
        "## ‚ö° Soluciones R√°pidas\n",
        "\n",
        "### Comandos de emergencia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emergency_fixes"
      },
      "outputs": [],
      "source": [
        "# üö® COMANDOS DE EMERGENCIA\n",
        "\n",
        "def emergency_reset():\n",
        "    \"\"\"Reset completo del entorno\"\"\"\n",
        "    print(\"üö® RESET DE EMERGENCIA\")\n",
        "    \n",
        "    # Limpiar memoria\n",
        "    import gc\n",
        "    import torch\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    # Variables de entorno\n",
        "    import os\n",
        "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "    \n",
        "    print(\"‚úÖ Reset completado\")\n",
        "    print(\"‚ö†Ô∏è Reinicia el runtime si los problemas persisten\")\n",
        "\n",
        "def check_environment():\n",
        "    \"\"\"Verificar entorno completo\"\"\"\n",
        "    print(\"üîç DIAGN√ìSTICO COMPLETO\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Python version\n",
        "    import sys\n",
        "    print(f\"Python: {sys.version}\")\n",
        "    \n",
        "    # PyTorch\n",
        "    try:\n",
        "        import torch\n",
        "        print(f\"PyTorch: {torch.__version__}\")\n",
        "        print(f\"CUDA: {torch.cuda.is_available()} ({torch.version.cuda if torch.cuda.is_available() else 'N/A'})\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå PyTorch no instalado\")\n",
        "    \n",
        "    # Transformers\n",
        "    try:\n",
        "        import transformers\n",
        "        print(f\"Transformers: {transformers.__version__}\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå Transformers no instalado\")\n",
        "    \n",
        "    # Unsloth\n",
        "    try:\n",
        "        import unsloth\n",
        "        print(f\"Unsloth: ‚úÖ Instalado\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå Unsloth no instalado\")\n",
        "    \n",
        "    # Protobuf\n",
        "    try:\n",
        "        import google.protobuf\n",
        "        print(f\"Protobuf: {google.protobuf.__version__}\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå Protobuf no instalado\")\n",
        "    \n",
        "    print(\"=\" * 40)\n",
        "\n",
        "# Ejecutar diagn√≥stico\n",
        "check_environment()\n",
        "emergency_reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prevention"
      },
      "source": [
        "## üõ°Ô∏è Prevenci√≥n de Errores\n",
        "\n",
        "### Mejores pr√°cticas para evitar problemas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "best_practices"
      },
      "outputs": [],
      "source": [
        "# üõ°Ô∏è MEJORES PR√ÅCTICAS\n",
        "\n",
        "def setup_best_practices():\n",
        "    \"\"\"Configuraci√≥n preventiva\"\"\"\n",
        "    print(\"üõ°Ô∏è Configurando mejores pr√°cticas...\")\n",
        "    \n",
        "    import os\n",
        "    import warnings\n",
        "    \n",
        "    # Variables de entorno preventivas\n",
        "    env_vars = {\n",
        "        \"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\": \"python\",\n",
        "        \"TOKENIZERS_PARALLELISM\": \"false\",\n",
        "        \"CUDA_LAUNCH_BLOCKING\": \"1\",  # Para debugging CUDA\n",
        "        \"PYTHONWARNINGS\": \"ignore\",   # Reducir warnings\n",
        "    }\n",
        "    \n",
        "    for key, value in env_vars.items():\n",
        "        os.environ[key] = value\n",
        "        print(f\"‚úÖ {key} = {value}\")\n",
        "    \n",
        "    # Suprimir warnings comunes\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    \n",
        "    print(\"\\nüìã CHECKLIST ANTES DE ENTRENAR:\")\n",
        "    checklist = [\n",
        "        \"‚úÖ Protobuf <= 3.20.3 instalado\",\n",
        "        \"‚úÖ Variables de entorno configuradas\",\n",
        "        \"‚úÖ GPU memory < 80% de uso\",\n",
        "        \"‚úÖ Batch size apropiado para tu GPU\",\n",
        "        \"‚úÖ Dataset cargado correctamente\",\n",
        "        \"‚úÖ Chat template configurado\",\n",
        "        \"‚úÖ Modelo y tokenizer en mismo dispositivo\"\n",
        "    ]\n",
        "    \n",
        "    for item in checklist:\n",
        "        print(f\"   {item}\")\n",
        "    \n",
        "    print(\"\\nüí° TIPS ADICIONALES:\")\n",
        "    tips = [\n",
        "        \"Siempre reinicia runtime despu√©s de instalar paquetes\",\n",
        "        \"Usa versiones espec√≠ficas en lugar de 'latest'\",\n",
        "        \"Guarda checkpoints frecuentemente\",\n",
        "        \"Monitorea uso de memoria durante entrenamiento\",\n",
        "        \"Prueba con datasets peque√±os primero\"\n",
        "    ]\n",
        "    \n",
        "    for tip in tips:\n",
        "        print(f\"   üí° {tip}\")\n",
        "\n",
        "# Ejecutar configuraci√≥n\n",
        "setup_best_practices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## üéØ Resumen de Soluciones\n",
        "\n",
        "### üîß Orden recomendado para solucionar problemas:\n",
        "\n",
        "1. **Ejecutar emergency_reset()** - Limpia memoria y configura variables\n",
        "2. **Instalar protobuf <= 3.20.3** - Soluciona el error m√°s com√∫n\n",
        "3. **Reinstalar Unsloth** - Con versiones compatibles\n",
        "4. **Reiniciar runtime** - Aplicar cambios\n",
        "5. **Verificar GPU memory** - Ajustar batch_size si es necesario\n",
        "6. **Probar con dataset peque√±o** - Validar configuraci√≥n\n",
        "\n",
        "### üìû Si nada funciona:\n",
        "- Usa Google Colab Pro para m√°s memoria\n",
        "- Prueba con CPU (m√°s lento pero funcional)\n",
        "- Considera usar modelos m√°s peque√±os\n",
        "- Revisa la documentaci√≥n oficial de Unsloth\n",
        "\n",
        "### üÜò Recursos de ayuda:\n",
        "- [Unsloth GitHub Issues](https://github.com/unslothai/unsloth/issues)\n",
        "- [Hugging Face Forum](https://discuss.huggingface.co/)\n",
        "- [PyTorch Troubleshooting](https://pytorch.org/docs/stable/notes/faq.html)\n",
        "\n",
        "---\n",
        "**Meta Day Uruguay 2025** - M√≥dulo 4: Troubleshooting üá∫üáæ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
