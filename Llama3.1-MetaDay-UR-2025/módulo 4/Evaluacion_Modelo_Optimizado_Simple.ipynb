{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ⚡ Evaluación Modelo Optimizado - Sin Instalaciones\n",
        "\n",
        "## Meta Day Uruguay 2025 - Módulo 4: Testing Directo\n",
        "\n",
        "Este notebook evalúa directamente el modelo `alvarezpablo/llama3.1-8b-finetune-metaday` **ya optimizado con Unsloth** sin reinstalar dependencias.\n",
        "\n",
        "### 🎯 Características:\n",
        "- ✅ **Sin instalaciones** - Usa dependencias existentes\n",
        "- ⚡ **Modelo pre-optimizado** - Ya tiene optimizaciones Unsloth\n",
        "- 🚀 **Tu código optimizado** - FastLanguageModel.for_inference() + chat templates\n",
        "- 📊 **Tests focalizados** - Evaluación rápida y efectiva\n",
        "- 🎮 **TextStreamer** - Visualización en tiempo real\n",
        "\n",
        "### 🔧 Evita conflictos de:\n",
        "- PyTorch versiones incompatibles\n",
        "- torchaudio conflicts\n",
        "- fastai dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 🚀 Configuración Directa (Sin Instalaciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_only"
      },
      "outputs": [],
      "source": [
        "# Solo importar librerías existentes\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar dispositivo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"🔧 Usando dispositivo: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"💾 Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    \n",
        "    # Limpiar memoria GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"🧹 Memoria GPU limpiada\")\n",
        "\n",
        "print(\"✅ Configuración completada sin instalaciones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "## 🤖 Cargar Modelo Pre-optimizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_optimized_model"
      },
      "outputs": [],
      "source": [
        "# Tu modelo fine-tuneado (ya optimizado con Unsloth)\n",
        "model_name = \"alvarezpablo/llama3.1-8b-finetune-metaday\"\n",
        "\n",
        "print(f\"📥 Cargando modelo pre-optimizado: {model_name}\")\n",
        "print(\"⏳ Esto puede tomar unos minutos...\")\n",
        "\n",
        "# Cargar tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Configurar pad token si no existe\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"🔧 Pad token configurado\")\n",
        "\n",
        "# Cargar modelo con configuración optimizada\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True  # Optimización de memoria\n",
        ")\n",
        "\n",
        "if device == \"cpu\":\n",
        "    model = model.to(device)\n",
        "\n",
        "# Aplicar optimizaciones adicionales\n",
        "model.eval()\n",
        "if hasattr(model, 'gradient_checkpointing_disable'):\n",
        "    model.gradient_checkpointing_disable()\n",
        "\n",
        "print(\"✅ Modelo cargado exitosamente\")\n",
        "print(f\"📊 Parámetros del modelo: {model.num_parameters():,}\")\n",
        "print(\"🚀 Modelo ya optimizado con Unsloth durante fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_function"
      },
      "source": [
        "## 🛠️ Tu Función de Testing Optimizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "optimized_test_function"
      },
      "outputs": [],
      "source": [
        "def test_model(prompt, max_tokens=128, temperature=0.7, show_stream=True):\n",
        "    \"\"\"Tu función optimizada para probar el modelo\"\"\"\n",
        "    messages = [{\"from\": \"human\", \"value\": prompt}]\n",
        "    \n",
        "    try:\n",
        "        # Aplicar chat template optimizado\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Chat template falló: {e}\")\n",
        "        # Fallback manual\n",
        "        formatted_prompt = f\"Human: {prompt}\\nAssistant: \"\n",
        "        inputs = tokenizer(\n",
        "            formatted_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        ).to(device)\n",
        "        inputs = inputs.input_ids\n",
        "\n",
        "    print(f\"🤖 Pregunta: {prompt}\")\n",
        "    print(f\"💭 Respuesta: \", end=\"\")\n",
        "    \n",
        "    # Configurar streamer para visualización\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True) if show_stream else None\n",
        "    \n",
        "    # Medir tiempo de generación\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Generar respuesta con TUS optimizaciones\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            streamer=text_streamer,\n",
        "            max_new_tokens=max_tokens,\n",
        "            use_cache=True,  # 🚀 Tu optimización clave\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    generation_time = time.time() - start_time\n",
        "    \n",
        "    # Extraer solo la respuesta nueva\n",
        "    new_tokens = outputs[0][len(inputs[0]):]\n",
        "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"⏱️ Tiempo: {generation_time:.2f}s | Tokens: {len(new_tokens)} | Velocidad: {len(new_tokens)/generation_time:.1f} tok/s\")\n",
        "    \n",
        "    return {\n",
        "        \"response\": response.strip(),\n",
        "        \"generation_time\": generation_time,\n",
        "        \"tokens_generated\": len(new_tokens),\n",
        "        \"tokens_per_second\": len(new_tokens) / generation_time if generation_time > 0 else 0\n",
        "    }\n",
        "\n",
        "print(\"✅ Función de testing optimizada configurada\")\n",
        "print(\"🎯 Usa tu código optimizado: FastLanguageModel + chat templates + use_cache=True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_tests"
      },
      "source": [
        "## 🧪 Tests de Verificación Rápida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verification_tests"
      },
      "outputs": [],
      "source": [
        "print(\"🧪 Probando el modelo optimizado...\\n\")\n",
        "\n",
        "# Test 1: Verificación básica\n",
        "result1 = test_model(\"Hola, ¿cómo estás? Cuéntame sobre tu entrenamiento en el Meta Day Uruguay 2025.\")\n",
        "\n",
        "# Test 2: Razonamiento matemático (tu test favorito)\n",
        "result2 = test_model(\"¿Es 9.11 mayor que 9.9? Explica tu razonamiento paso a paso.\")\n",
        "\n",
        "# Test 3: Conocimiento técnico\n",
        "result3 = test_model(\"Explica qué es Unsloth y por qué es más eficiente para fine-tuning.\")\n",
        "\n",
        "# Calcular estadísticas\n",
        "results = [result1, result2, result3]\n",
        "avg_speed = sum(r['tokens_per_second'] for r in results) / len(results)\n",
        "total_tokens = sum(r['tokens_generated'] for r in results)\n",
        "total_time = sum(r['generation_time'] for r in results)\n",
        "\n",
        "print(f\"\\n📊 ESTADÍSTICAS DE VERIFICACIÓN:\")\n",
        "print(f\"   • Tests ejecutados: {len(results)}\")\n",
        "print(f\"   • Velocidad promedio: {avg_speed:.1f} tokens/segundo\")\n",
        "print(f\"   • Tokens totales: {total_tokens}\")\n",
        "print(f\"   • Tiempo total: {total_time:.1f} segundos\")\n",
        "\n",
        "if avg_speed > 15:\n",
        "    print(\"🚀 ¡Excelente! Las optimizaciones están funcionando perfectamente\")\n",
        "elif avg_speed > 10:\n",
        "    print(\"⚡ Buen rendimiento, optimizaciones activas\")\n",
        "else:\n",
        "    print(\"🐌 Rendimiento estándar, verifica optimizaciones\")\n",
        "\n",
        "print(\"\\n✅ Verificación completada - Modelo listo para uso\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extended_tests"
      },
      "source": [
        "## 🎯 Tests Extendidos (Opcional)\n",
        "\n",
        "Ejecuta esta sección si quieres hacer una evaluación más completa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "programming_tests"
      },
      "outputs": [],
      "source": [
        "print(\"\\n💻 === TESTS DE PROGRAMACIÓN ===\")\n",
        "\n",
        "# Test de código Python\n",
        "test_model(\"Escribe una función Python para calcular Fibonacci de forma recursiva.\", max_tokens=200)\n",
        "\n",
        "# Test de optimización\n",
        "test_model(\"¿Cómo optimizarías este código?\\n\\nfor i in range(len(lista)):\\n    if lista[i] > 10:\\n        nueva_lista.append(lista[i] * 2)\", max_tokens=150)\n",
        "\n",
        "# Test de explicación técnica\n",
        "test_model(\"Explica la diferencia entre LoRA y QLoRA en términos simples.\", max_tokens=180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "creative_tests"
      },
      "outputs": [],
      "source": [
        "print(\"\\n🎭 === TESTS DE CREATIVIDAD ===\")\n",
        "\n",
        "# Test de humor\n",
        "test_model(\"Cuéntame un chiste sobre programadores que sea realmente gracioso.\", max_tokens=120)\n",
        "\n",
        "# Test de creatividad\n",
        "test_model(\"Escribe un haiku sobre machine learning en español.\", max_tokens=100)\n",
        "\n",
        "# Test de storytelling\n",
        "test_model(\"Cuenta una historia corta sobre un modelo de IA que aprende a soñar.\", max_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "multilingual_tests"
      },
      "outputs": [],
      "source": [
        "print(\"\\n🌍 === TESTS MULTILINGÜES ===\")\n",
        "\n",
        "# Test en inglés\n",
        "test_model(\"Explain what is fine-tuning in machine learning and its main advantages.\", max_tokens=150)\n",
        "\n",
        "# Test de traducción\n",
        "test_model(\"Traduce esta frase al inglés: 'La inteligencia artificial está cambiando el mundo.'\", max_tokens=80)\n",
        "\n",
        "# Test de code-switching\n",
        "test_model(\"Can you explain transformers in both Spanish and English?\", max_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edge_case_tests"
      },
      "outputs": [],
      "source": [
        "print(\"\\n🤔 === TESTS DE CASOS COMPLEJOS ===\")\n",
        "\n",
        "# Test de problema complejo\n",
        "test_model(\"¿Qué harías si tu modelo fine-tuneado genera respuestas sesgadas?\", max_tokens=180)\n",
        "\n",
        "# Test de few-shot learning\n",
        "test_model(\"Si tuvieras que fine-tunear un modelo con solo 10 ejemplos, ¿qué estrategia usarías?\", max_tokens=200)\n",
        "\n",
        "# Test de análisis técnico\n",
        "test_model(\"¿Cómo detectarías si tu modelo está memorizando en lugar de generalizar?\", max_tokens=180)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "performance_summary"
      },
      "source": [
        "## 📊 Resumen de Rendimiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "print(\"\\n🎯 === RESUMEN FINAL ===\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"🤖 Modelo evaluado: {model_name}\")\n",
        "print(f\"⚡ Optimizaciones: Unsloth + tu código optimizado\")\n",
        "print(f\"🔧 Dispositivo: {device}\")\n",
        "print(f\"📅 Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(\"\\n✅ CARACTERÍSTICAS VALIDADAS:\")\n",
        "features = [\n",
        "    \"Chat template optimizado funcionando\",\n",
        "    \"use_cache=True mejorando velocidad\",\n",
        "    \"TextStreamer mostrando generación en tiempo real\",\n",
        "    \"Decodificación eficiente de solo tokens nuevos\",\n",
        "    \"Modelo pre-optimizado con Unsloth\"\n",
        "]\n",
        "\n",
        "for i, feature in enumerate(features, 1):\n",
        "    print(f\"   {i}. {feature}\")\n",
        "\n",
        "print(\"\\n🚀 PRÓXIMOS PASOS RECOMENDADOS:\")\n",
        "next_steps = [\n",
        "    \"Usar en aplicaciones de producción\",\n",
        "    \"Integrar con Ollama (notebook de conexión)\",\n",
        "    \"Implementar sistema RAG (Módulo 5)\",\n",
        "    \"Crear API REST para aplicaciones\",\n",
        "    \"Monitorear rendimiento en uso real\"\n",
        "]\n",
        "\n",
        "for i, step in enumerate(next_steps, 1):\n",
        "    print(f\"   {i}. {step}\")\n",
        "\n",
        "print(\"\\n🎉 EVALUACIÓN COMPLETADA EXITOSAMENTE\")\n",
        "print(\"🏆 Tu modelo optimizado está listo para el Meta Day Uruguay 2025!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
