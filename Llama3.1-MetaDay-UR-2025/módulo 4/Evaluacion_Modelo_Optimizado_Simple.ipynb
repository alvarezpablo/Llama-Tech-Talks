{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ‚ö° Evaluaci√≥n Modelo Optimizado - Sin Instalaciones\n",
        "\n",
        "## Meta Day Uruguay 2025 - M√≥dulo 4: Testing Directo\n",
        "\n",
        "Este notebook eval√∫a directamente el modelo `alvarezpablo/llama3.1-8b-finetune-metaday` **ya optimizado con Unsloth** sin reinstalar dependencias.\n",
        "\n",
        "### üéØ Caracter√≠sticas:\n",
        "- ‚úÖ **Sin instalaciones** - Usa dependencias existentes\n",
        "- ‚ö° **Modelo pre-optimizado** - Ya tiene optimizaciones Unsloth\n",
        "- üöÄ **Tu c√≥digo optimizado** - FastLanguageModel.for_inference() + chat templates\n",
        "- üìä **Tests focalizados** - Evaluaci√≥n r√°pida y efectiva\n",
        "- üéÆ **TextStreamer** - Visualizaci√≥n en tiempo real\n",
        "\n",
        "### üîß Evita conflictos de:\n",
        "- PyTorch versiones incompatibles\n",
        "- torchaudio conflicts\n",
        "- fastai dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üöÄ Configuraci√≥n Directa (Sin Instalaciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_only"
      },
      "outputs": [],
      "source": [
        "# Solo importar librer√≠as existentes\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar dispositivo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üîß Usando dispositivo: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    \n",
        "    # Limpiar memoria GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"üßπ Memoria GPU limpiada\")\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n completada sin instalaciones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "## ü§ñ Cargar Modelo Pre-optimizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_optimized_model"
      },
      "outputs": [],
      "source": [
        "# Tu modelo fine-tuneado (ya optimizado con Unsloth)\n",
        "model_name = \"alvarezpablo/llama3.1-8b-finetune-metaday\"\n",
        "\n",
        "print(f\"üì• Cargando modelo pre-optimizado: {model_name}\")\n",
        "print(\"‚è≥ Esto puede tomar unos minutos...\")\n",
        "\n",
        "# Cargar tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Configurar pad token si no existe\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"üîß Pad token configurado\")\n",
        "\n",
        "# Cargar modelo con configuraci√≥n optimizada\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True  # Optimizaci√≥n de memoria\n",
        ")\n",
        "\n",
        "if device == \"cpu\":\n",
        "    model = model.to(device)\n",
        "\n",
        "# Aplicar optimizaciones adicionales\n",
        "model.eval()\n",
        "if hasattr(model, 'gradient_checkpointing_disable'):\n",
        "    model.gradient_checkpointing_disable()\n",
        "\n",
        "print(\"‚úÖ Modelo cargado exitosamente\")\n",
        "print(f\"üìä Par√°metros del modelo: {model.num_parameters():,}\")\n",
        "print(\"üöÄ Modelo ya optimizado con Unsloth durante fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_function"
      },
      "source": [
        "## üõ†Ô∏è Tu Funci√≥n de Testing Optimizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "optimized_test_function"
      },
      "outputs": [],
      "source": [
        "def test_model(prompt, max_tokens=128, temperature=0.7, show_stream=True):\n",
        "    \"\"\"Tu funci√≥n optimizada para probar el modelo\"\"\"\n",
        "    messages = [{\"from\": \"human\", \"value\": prompt}]\n",
        "    \n",
        "    try:\n",
        "        # Aplicar chat template optimizado\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Chat template fall√≥: {e}\")\n",
        "        # Fallback manual\n",
        "        formatted_prompt = f\"Human: {prompt}\\nAssistant: \"\n",
        "        inputs = tokenizer(\n",
        "            formatted_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        ).to(device)\n",
        "        inputs = inputs.input_ids\n",
        "\n",
        "    print(f\"ü§ñ Pregunta: {prompt}\")\n",
        "    print(f\"üí≠ Respuesta: \", end=\"\")\n",
        "    \n",
        "    # Configurar streamer para visualizaci√≥n\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True) if show_stream else None\n",
        "    \n",
        "    # Medir tiempo de generaci√≥n\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Generar respuesta con TUS optimizaciones\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            streamer=text_streamer,\n",
        "            max_new_tokens=max_tokens,\n",
        "            use_cache=True,  # üöÄ Tu optimizaci√≥n clave\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    generation_time = time.time() - start_time\n",
        "    \n",
        "    # Extraer solo la respuesta nueva\n",
        "    new_tokens = outputs[0][len(inputs[0]):]\n",
        "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"‚è±Ô∏è Tiempo: {generation_time:.2f}s | Tokens: {len(new_tokens)} | Velocidad: {len(new_tokens)/generation_time:.1f} tok/s\")\n",
        "    \n",
        "    return {\n",
        "        \"response\": response.strip(),\n",
        "        \"generation_time\": generation_time,\n",
        "        \"tokens_generated\": len(new_tokens),\n",
        "        \"tokens_per_second\": len(new_tokens) / generation_time if generation_time > 0 else 0\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Funci√≥n de testing optimizada configurada\")\n",
        "print(\"üéØ Usa tu c√≥digo optimizado: FastLanguageModel + chat templates + use_cache=True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_tests"
      },
      "source": [
        "## üß™ Tests de Verificaci√≥n R√°pida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verification_tests"
      },
      "outputs": [],
      "source": [
        "print(\"üß™ Probando el modelo optimizado...\\n\")\n",
        "\n",
        "# Test 1: Verificaci√≥n b√°sica\n",
        "result1 = test_model(\"Hola, ¬øc√≥mo est√°s? Cu√©ntame sobre tu entrenamiento en el Meta Day Uruguay 2025.\")\n",
        "\n",
        "# Test 2: Razonamiento matem√°tico (tu test favorito)\n",
        "result2 = test_model(\"¬øEs 9.11 mayor que 9.9? Explica tu razonamiento paso a paso.\")\n",
        "\n",
        "# Test 3: Conocimiento t√©cnico\n",
        "result3 = test_model(\"Explica qu√© es Unsloth y por qu√© es m√°s eficiente para fine-tuning.\")\n",
        "\n",
        "# Calcular estad√≠sticas\n",
        "results = [result1, result2, result3]\n",
        "avg_speed = sum(r['tokens_per_second'] for r in results) / len(results)\n",
        "total_tokens = sum(r['tokens_generated'] for r in results)\n",
        "total_time = sum(r['generation_time'] for r in results)\n",
        "\n",
        "print(f\"\\nüìä ESTAD√çSTICAS DE VERIFICACI√ìN:\")\n",
        "print(f\"   ‚Ä¢ Tests ejecutados: {len(results)}\")\n",
        "print(f\"   ‚Ä¢ Velocidad promedio: {avg_speed:.1f} tokens/segundo\")\n",
        "print(f\"   ‚Ä¢ Tokens totales: {total_tokens}\")\n",
        "print(f\"   ‚Ä¢ Tiempo total: {total_time:.1f} segundos\")\n",
        "\n",
        "if avg_speed > 15:\n",
        "    print(\"üöÄ ¬°Excelente! Las optimizaciones est√°n funcionando perfectamente\")\n",
        "elif avg_speed > 10:\n",
        "    print(\"‚ö° Buen rendimiento, optimizaciones activas\")\n",
        "else:\n",
        "    print(\"üêå Rendimiento est√°ndar, verifica optimizaciones\")\n",
        "\n",
        "print(\"\\n‚úÖ Verificaci√≥n completada - Modelo listo para uso\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extended_tests"
      },
      "source": [
        "## üéØ Tests Extendidos (Opcional)\n",
        "\n",
        "Ejecuta esta secci√≥n si quieres hacer una evaluaci√≥n m√°s completa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "programming_tests"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüíª === TESTS DE PROGRAMACI√ìN ===\")\n",
        "\n",
        "# Test de c√≥digo Python\n",
        "test_model(\"Escribe una funci√≥n Python para calcular Fibonacci de forma recursiva.\", max_tokens=200)\n",
        "\n",
        "# Test de optimizaci√≥n\n",
        "test_model(\"¬øC√≥mo optimizar√≠as este c√≥digo?\\n\\nfor i in range(len(lista)):\\n    if lista[i] > 10:\\n        nueva_lista.append(lista[i] * 2)\", max_tokens=150)\n",
        "\n",
        "# Test de explicaci√≥n t√©cnica\n",
        "test_model(\"Explica la diferencia entre LoRA y QLoRA en t√©rminos simples.\", max_tokens=180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "creative_tests"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüé≠ === TESTS DE CREATIVIDAD ===\")\n",
        "\n",
        "# Test de humor\n",
        "test_model(\"Cu√©ntame un chiste sobre programadores que sea realmente gracioso.\", max_tokens=120)\n",
        "\n",
        "# Test de creatividad\n",
        "test_model(\"Escribe un haiku sobre machine learning en espa√±ol.\", max_tokens=100)\n",
        "\n",
        "# Test de storytelling\n",
        "test_model(\"Cuenta una historia corta sobre un modelo de IA que aprende a so√±ar.\", max_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "multilingual_tests"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüåç === TESTS MULTILING√úES ===\")\n",
        "\n",
        "# Test en ingl√©s\n",
        "test_model(\"Explain what is fine-tuning in machine learning and its main advantages.\", max_tokens=150)\n",
        "\n",
        "# Test de traducci√≥n\n",
        "test_model(\"Traduce esta frase al ingl√©s: 'La inteligencia artificial est√° cambiando el mundo.'\", max_tokens=80)\n",
        "\n",
        "# Test de code-switching\n",
        "test_model(\"Can you explain transformers in both Spanish and English?\", max_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edge_case_tests"
      },
      "outputs": [],
      "source": [
        "print(\"\\nü§î === TESTS DE CASOS COMPLEJOS ===\")\n",
        "\n",
        "# Test de problema complejo\n",
        "test_model(\"¬øQu√© har√≠as si tu modelo fine-tuneado genera respuestas sesgadas?\", max_tokens=180)\n",
        "\n",
        "# Test de few-shot learning\n",
        "test_model(\"Si tuvieras que fine-tunear un modelo con solo 10 ejemplos, ¬øqu√© estrategia usar√≠as?\", max_tokens=200)\n",
        "\n",
        "# Test de an√°lisis t√©cnico\n",
        "test_model(\"¬øC√≥mo detectar√≠as si tu modelo est√° memorizando en lugar de generalizar?\", max_tokens=180)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "performance_summary"
      },
      "source": [
        "## üìä Resumen de Rendimiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüéØ === RESUMEN FINAL ===\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"ü§ñ Modelo evaluado: {model_name}\")\n",
        "print(f\"‚ö° Optimizaciones: Unsloth + tu c√≥digo optimizado\")\n",
        "print(f\"üîß Dispositivo: {device}\")\n",
        "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(\"\\n‚úÖ CARACTER√çSTICAS VALIDADAS:\")\n",
        "features = [\n",
        "    \"Chat template optimizado funcionando\",\n",
        "    \"use_cache=True mejorando velocidad\",\n",
        "    \"TextStreamer mostrando generaci√≥n en tiempo real\",\n",
        "    \"Decodificaci√≥n eficiente de solo tokens nuevos\",\n",
        "    \"Modelo pre-optimizado con Unsloth\"\n",
        "]\n",
        "\n",
        "for i, feature in enumerate(features, 1):\n",
        "    print(f\"   {i}. {feature}\")\n",
        "\n",
        "print(\"\\nüöÄ PR√ìXIMOS PASOS RECOMENDADOS:\")\n",
        "next_steps = [\n",
        "    \"Usar en aplicaciones de producci√≥n\",\n",
        "    \"Integrar con Ollama (notebook de conexi√≥n)\",\n",
        "    \"Implementar sistema RAG (M√≥dulo 5)\",\n",
        "    \"Crear API REST para aplicaciones\",\n",
        "    \"Monitorear rendimiento en uso real\"\n",
        "]\n",
        "\n",
        "for i, step in enumerate(next_steps, 1):\n",
        "    print(f\"   {i}. {step}\")\n",
        "\n",
        "print(\"\\nüéâ EVALUACI√ìN COMPLETADA EXITOSAMENTE\")\n",
        "print(\"üèÜ Tu modelo optimizado est√° listo para el Meta Day Uruguay 2025!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
