{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Módulo 2: Introducción a Llama 3.1\n\n## Demostración práctica de las capacidades y características de Llama 3.1\n\n**Autor:** Pablo Álvarez \n**Fecha:** Julio 2025\n\n---\n\n### 🚀 **Compatible con Kaggle**\n\nEste notebook está optimizado para ejecutarse en:\n- **Kaggle Notebooks** (recomendado para GPU gratuita)\n- **Google Colab**\n- **Entorno local**\n\n### 🔑 **Configuración de Token HF (Variables de Ambiente):**\n\nEste notebook lee el token de Hugging Face directamente desde las **variables de ambiente del sistema operativo**:\n\n**Para Kaggle:**\n1. Ve a **Settings > Secrets** en tu notebook\n2. Agrega `HF_TOKEN` con tu token de Hugging Face\n3. El notebook lo detectará automáticamente\n\n**Para Google Colab:**\n1. Ejecuta: `import os`\n2. Ejecuta: `os.environ['HF_TOKEN'] = 'tu_token_aqui'`\n\n**Para entorno local:**\n1. `export HF_TOKEN=tu_token_aqui`\n2. O agrega al archivo `.bashrc`/`.zshrc`\n\n**Obtener token:** https://huggingface.co/settings/tokens\n\n### 🚀 **Activar GPU (Kaggle):**\n- Ve a **Settings > Accelerator**\n- Selecciona **GPU T4 x2** (gratuito)\n\n---\n\nEn este notebook exploraremos las capacidades de Llama 3.1, incluyendo:\n\n1. **Carga del modelo** - Configuración básica y cuantizada\n2. **Capacidades básicas** - Generación de texto general\n3. **Capacidades conversacionales** - Formato de chat\n4. **Capacidades multilingües** - Soporte para múltiples idiomas\n5. **Generación de código** - Programación asistida por IA\n6. **Análisis de rendimiento** - Métricas y optimización\n7. **Comparación de configuraciones** - Diferentes parámetros de generación","metadata":{}},{"cell_type":"markdown","source":"## 1. Verificación del Entorno y Configuración","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:09:51.710107Z","iopub.execute_input":"2025-08-01T02:09:51.710345Z","iopub.status.idle":"2025-08-01T02:11:12.398736Z","shell.execute_reply.started":"2025-08-01T02:09:51.710328Z","shell.execute_reply":"2025-08-01T02:11:12.397839Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Verificar entorno y configuración\nimport os\nimport sys\n\n# Detectar entorno\ndef detectar_entorno():\n    if os.path.exists('/kaggle/input') or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n        return 'Kaggle'\n    elif 'COLAB_GPU' in os.environ:\n        return 'Google Colab'\n    else:\n        return 'Local'\n\nentorno = detectar_entorno()\nprint(f\"🔍 Entorno detectado: {entorno}\")\n\nfrom kaggle_secrets import UserSecretsClient\n\n# Accede el secreto \"HF_TOKEN\"\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n# (Opcional) lo setea como variable de entorno para que Transformers lo tome automáticamente\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token  # A veces esta variable también es requerida\n\nprint(\"Token HF seteado correctamente:\", hf_token[:10], \"...\")\n\n# Verificar GPU\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"🚀 GPU disponible: {gpu_name}\")\n    print(f\"💾 Memoria GPU: {gpu_memory:.1f} GB\")\nelse:\n    print(\"⚠️ GPU no disponible - usando CPU\")\n\nprint(f\"🐍 Python: {sys.version}\")\nprint(f\"🔥 PyTorch: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:15.915723Z","iopub.execute_input":"2025-08-01T02:12:15.916078Z","iopub.status.idle":"2025-08-01T02:12:17.812848Z","shell.execute_reply.started":"2025-08-01T02:12:15.916048Z","shell.execute_reply":"2025-08-01T02:12:17.812109Z"}},"outputs":[{"name":"stdout","text":"🔍 Entorno detectado: Kaggle\nToken HF seteado correctamente: hf_FLjugcm ...\n🚀 GPU disponible: Tesla T4\n💾 Memoria GPU: 14.7 GB\n🐍 Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n🔥 PyTorch: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 2. Configuración Manual del Token (Opcional)\n\nSi necesitas configurar el token manualmente en el notebook, ejecuta la siguiente celda:","metadata":{}},{"cell_type":"code","source":"# OPCIONAL: Configurar token manualmente si no está en variables de ambiente\n# Descomenta y ejecuta solo si es necesario\n\n# import os\n# \n# # Reemplaza 'tu_token_aqui' con tu token real de Hugging Face\n# # os.environ['HF_TOKEN'] = 'tu_token_aqui'\n# \n# # Verificar que se configuró correctamente\n# if os.getenv('HF_TOKEN'):\n#     print(f\"✅ Token configurado: {os.getenv('HF_TOKEN')[:10]}...\")\n# else:\n#     print(\"⚠️ Token no configurado\")\n\nprint(\"💡 Esta celda es opcional. Solo úsala si el token no se detecta automáticamente.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:22.080181Z","iopub.execute_input":"2025-08-01T02:12:22.080840Z","iopub.status.idle":"2025-08-01T02:12:22.084727Z","shell.execute_reply.started":"2025-08-01T02:12:22.080800Z","shell.execute_reply":"2025-08-01T02:12:22.084084Z"}},"outputs":[{"name":"stdout","text":"💡 Esta celda es opcional. Solo úsala si el token no se detecta automáticamente.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Importaciones y Configuración Inicial","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\nimport torch\nimport time\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import List, Dict, Optional\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Detectar entorno\ndef is_kaggle():\n    return os.path.exists('/kaggle/input') or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n\ndef is_colab():\n    return 'COLAB_GPU' in os.environ\n\n# Configurar paths según el entorno\nif is_kaggle():\n    print(\"🔍 Ejecutando en Kaggle\")\n    # En Kaggle, el código puede estar en /kaggle/input/\n    config_paths = ['/kaggle/input/cursollama311/config', './config', '../config']\nelif is_colab():\n    print(\"🔍 Ejecutando en Google Colab\")\n    config_paths = ['./config', '../config']\nelse:\n    print(\"🔍 Ejecutando en entorno local\")\n    config_paths = ['./config', '../config', str(Path.cwd().parent / 'config')]\n\n# Agregar paths de configuración\nfor path in config_paths:\n    if os.path.exists(path):\n        sys.path.insert(0, path)\n        break\n\n# Instalar dependencias si es necesario (para Kaggle/Colab)\nif is_kaggle() or is_colab():\n    try:\n        import transformers\n        print(f\"✅ Transformers {transformers.__version__} ya instalado\")\n    except ImportError:\n        print(\"📦 Instalando transformers...\")\n        %pip install -q transformers accelerate bitsandbytes\n\n# Importar librerías principales\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    pipeline\n)\n\n# Importar psutil con fallback\ntry:\n    import psutil\n    HAS_PSUTIL = True\nexcept ImportError:\n    print(\"⚠️ psutil no disponible - métricas de memoria limitadas\")\n    HAS_PSUTIL = False\n\n# Importar matplotlib con fallback\ntry:\n    import matplotlib.pyplot as plt\n    HAS_MATPLOTLIB = True\nexcept ImportError:\n    print(\"⚠️ matplotlib no disponible - sin gráficos\")\n    HAS_MATPLOTLIB = False\n\n# Configuración simplificada usando variables de ambiente del sistema\ndef setup_huggingface_auth():\n    \"\"\"Configurar autenticación usando variables de ambiente del sistema operativo\"\"\"\n    # Buscar token en múltiples variables de ambiente\n    token = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n    \n    if token:\n        # Configurar todas las variables que transformers puede usar\n        os.environ['HUGGING_FACE_HUB_TOKEN'] = token\n        os.environ['HF_TOKEN'] = token\n        print(\"🔐 Autenticación HF configurada desde variables de ambiente del sistema\")\n        return True\n    else:\n        print(\"⚠️ No se encontró token HF en variables de ambiente del sistema\")\n        return False\n\ndef get_model_config():\n    \"\"\"Obtener configuración de modelo desde variables de ambiente\"\"\"\n    return {\n        'default_model': os.getenv('DEFAULT_MODEL', 'meta-llama/Meta-Llama-3.1-8B-Instruct'),\n        'fallback_model': os.getenv('FALLBACK_MODEL', 'microsoft/DialoGPT-medium'),\n        'cache_dir': os.getenv('HF_CACHE_DIR', '/tmp/huggingface_cache' if is_kaggle() else './models_cache'),\n        'device': os.getenv('DEVICE', 'auto'),\n        'use_quantization': os.getenv('USE_QUANTIZATION', 'true').lower() == 'true'\n    }\n\n# Configurar autenticación automáticamente al importar\nauth_success = setup_huggingface_auth()\nmodel_config = get_model_config()\n\nprint(f\"🔧 Configuración desde variables de ambiente:\")\nprint(f\"   Autenticación: {'✅ Configurada' if auth_success else '❌ No configurada'}\")\nprint(f\"   Modelo: {model_config['default_model']}\")\nprint(f\"   Cache: {model_config['cache_dir']}\")\nprint(f\"   Cuantización: {model_config['use_quantization']}\")\n\nprint(\"📦 Librerías importadas correctamente\")\nprint(f\"🔥 PyTorch: {torch.__version__}\")\nprint(f\"🤗 Transformers: {transformers.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:25.922708Z","iopub.execute_input":"2025-08-01T02:12:25.923290Z","iopub.status.idle":"2025-08-01T02:12:48.450487Z","shell.execute_reply.started":"2025-08-01T02:12:25.923265Z","shell.execute_reply":"2025-08-01T02:12:48.449786Z"}},"outputs":[{"name":"stdout","text":"🔍 Ejecutando en Kaggle\n✅ Transformers 4.52.4 ya instalado\n","output_type":"stream"},{"name":"stderr","text":"2025-08-01 02:12:34.755450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754014354.969193      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754014355.031275      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🔐 Autenticación HF configurada desde variables de ambiente del sistema\n🔧 Configuración desde variables de ambiente:\n   Autenticación: ✅ Configurada\n   Modelo: meta-llama/Meta-Llama-3.1-8B-Instruct\n   Cache: /tmp/huggingface_cache\n   Cuantización: True\n📦 Librerías importadas correctamente\n🔥 PyTorch: 2.6.0+cu124\n🤗 Transformers: 4.52.4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2. Definición de la Clase Llama31Demo","metadata":{}},{"cell_type":"code","source":"class Llama31Demo:\n    \"\"\"Clase para demostrar las capacidades de Llama 3.1\"\"\"\n    \n    def __init__(self, model_name: str = None):\n        # Usar configuración desde variables de ambiente del sistema\n        config = get_model_config()\n        self.model_name = model_name or config['default_model']\n        self.cache_dir = config['cache_dir']\n        self.use_quantization = config['use_quantization']\n        \n        self.tokenizer = None\n        self.model = None\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        print(f\"🦙 Inicializando demostración de Llama 3.1\")\n        print(f\"📱 Dispositivo: {self.device}\")\n        print(f\"🤖 Modelo: {self.model_name}\")\n        print(f\"📁 Cache: {self.cache_dir}\")\n        print(f\"⚡ Cuantización: {self.use_quantization}\")\n    \n    def _obtener_uso_memoria(self) -> float:\n        \"\"\"Obtener uso actual de memoria en MB\"\"\"\n        process = psutil.Process(os.getpid())\n        return process.memory_info().rss / (1024 * 1024)\n    \n    def _mostrar_info_modelo(self):\n        \"\"\"Mostrar información detallada del modelo\"\"\"\n        if self.model is None:\n            return\n        \n        # Contar parámetros\n        total_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        \n        print(f\"\\n📊 Información del modelo:\")\n        print(f\"   Nombre: {self.model_name}\")\n        print(f\"   Parámetros totales: {total_params:,}\")\n        print(f\"   Parámetros entrenables: {trainable_params:,}\")\n        print(f\"   Dispositivo: {next(self.model.parameters()).device}\")\n        print(f\"   Tipo de datos: {next(self.model.parameters()).dtype}\")\n        \n        # Uso de memoria\n        memoria_mb = self._obtener_uso_memoria()\n        print(f\"   Memoria utilizada: {memoria_mb:.2f} MB\")\n\n# Crear instancia de la demo\ndemo = Llama31Demo()\nprint(\"✅ Instancia de Llama31Demo creada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:48.499170Z","iopub.execute_input":"2025-08-01T02:12:48.499456Z","iopub.status.idle":"2025-08-01T02:12:48.513153Z","shell.execute_reply.started":"2025-08-01T02:12:48.499433Z","shell.execute_reply":"2025-08-01T02:12:48.512590Z"}},"outputs":[{"name":"stdout","text":"🦙 Inicializando demostración de Llama 3.1\n📱 Dispositivo: cuda\n🤖 Modelo: meta-llama/Meta-Llama-3.1-8B-Instruct\n📁 Cache: /tmp/huggingface_cache\n⚡ Cuantización: True\n✅ Instancia de Llama31Demo creada\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 3. Carga del Modelo - Configuración Cuantizada\n\nPrimero intentaremos cargar el modelo con cuantización 4-bit para optimizar el uso de memoria.","metadata":{}},{"cell_type":"code","source":"def cargar_modelo_cuantizado(self):\n    \"\"\"Cargar Llama 3.1 con cuantización para eficiencia\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"⚡ CARGANDO LLAMA 3.1 - CONFIGURACIÓN CUANTIZADA\")\n    print(\"=\"*60)\n    \n    try:\n        # Configuración de cuantización\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n        \n        print(\"🔧 Configuración de cuantización 4-bit activada\")\n        \n        # Cargar tokenizer usando token desde variables de ambiente\n        if self.tokenizer is None:\n            hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.model_name,\n                token=hf_token,\n                cache_dir=self.cache_dir\n            )\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Cargar modelo cuantizado\n        print(\"🧠 Cargando modelo cuantizado...\")\n        start_time = time.time()\n        \n        hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            token=hf_token,\n            cache_dir=self.cache_dir\n        )\n        \n        load_time = time.time() - start_time\n        \n        print(f\"✅ Modelo cuantizado cargado en {load_time:.2f} segundos\")\n        print(\"💾 Uso de memoria significativamente reducido\")\n        \n        # Mostrar información\n        self._mostrar_info_modelo()\n        \n    except Exception as e:\n        print(f\"❌ Error cargando modelo cuantizado: {e}\")\n        if \"bitsandbytes\" in str(e):\n            print(\"💡 Instalación requerida: pip install bitsandbytes\")\n        print(\"🔄 Cargando modelo básico...\")\n        return False\n    return True\n\n# Agregar método a la instancia\nLlama31Demo.cargar_modelo_cuantizado = cargar_modelo_cuantizado\n\n# Intentar cargar modelo cuantizado\nprint(\"🔄 Intentando cargar modelo cuantizado...\")\nexito_cuantizado = demo.cargar_modelo_cuantizado()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:53.815291Z","iopub.execute_input":"2025-08-01T02:12:53.815536Z","iopub.status.idle":"2025-08-01T02:15:54.591765Z","shell.execute_reply.started":"2025-08-01T02:12:53.815519Z","shell.execute_reply":"2025-08-01T02:15:54.591104Z"}},"outputs":[{"name":"stdout","text":"🔄 Intentando cargar modelo cuantizado...\n\n============================================================\n⚡ CARGANDO LLAMA 3.1 - CONFIGURACIÓN CUANTIZADA\n============================================================\n🔧 Configuración de cuantización 4-bit activada\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fa8c97cb69f4ac5a9bebde2c24657ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551d166803b64c819267e92d758b28b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71bfd097f4664d64bb4abc5d5bf2dd07"}},"metadata":{}},{"name":"stdout","text":"🧠 Cargando modelo cuantizado...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23dfb0bf3ca6497988a3e2e889cb65c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d392ac66b4040659e268d4ad8c4554c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbf1d712953405696e83dab985358e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8d41ea06564432be41ae7a0d61cf02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27687907a0d649c8ace528b02814ab43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8e9e6de3dd4386bd195575b5f4dab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6478f687b6b44cab39f4601bf482620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"781390b034ee48bcb5e6f5dcd55cb799"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5701cf7ae42e4172910a72da8435bd2c"}},"metadata":{}},{"name":"stdout","text":"✅ Modelo cuantizado cargado en 178.42 segundos\n💾 Uso de memoria significativamente reducido\n\n📊 Información del modelo:\n   Nombre: meta-llama/Meta-Llama-3.1-8B-Instruct\n   Parámetros totales: 4,540,600,320\n   Parámetros entrenables: 1,050,939,392\n   Dispositivo: cuda:0\n   Tipo de datos: torch.float16\n   Memoria utilizada: 3277.27 MB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 4. Carga del Modelo - Configuración Básica\n\nSi la cuantización falla, cargaremos el modelo en configuración básica.","metadata":{}},{"cell_type":"code","source":"def cargar_modelo_basico(self):\n    \"\"\"Cargar Llama 3.1 en configuración básica\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"📥 CARGANDO LLAMA 3.1 - CONFIGURACIÓN BÁSICA\")\n    print(\"=\"*60)\n    \n    try:\n        # Cargar tokenizer usando token desde variables de ambiente\n        print(\"🔤 Cargando tokenizer...\")\n        hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_name,\n            token=hf_token,\n            cache_dir=self.cache_dir\n        )\n        \n        # Configurar pad token si no existe\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        print(f\"✅ Tokenizer cargado\")\n        print(f\"   Vocabulario: {self.tokenizer.vocab_size:,} tokens\")\n        print(f\"   Tokens especiales: {len(self.tokenizer.special_tokens_map)}\")\n        \n        # Cargar modelo\n        print(\"🧠 Cargando modelo...\")\n        start_time = time.time()\n        \n        hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n            device_map=\"auto\" if self.device == \"cuda\" else \"cpu\",\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            token=hf_token,\n            cache_dir=self.cache_dir\n        )\n        \n        load_time = time.time() - start_time\n        \n        print(f\"✅ Modelo cargado en {load_time:.2f} segundos\")\n        \n        # Información del modelo\n        self._mostrar_info_modelo()\n        \n    except Exception as e:\n        print(f\"❌ Error cargando modelo: {e}\")\n        print(\"💡 Intentando con modelo alternativo...\")\n\n        # Intentar con modelo alternativo\n        try:\n            fallback_model = \"microsoft/DialoGPT-medium\"\n            print(f\"🔄 Cargando modelo alternativo: {fallback_model}\")\n\n            self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n\n            self.model = AutoModelForCausalLM.from_pretrained(\n                fallback_model,\n                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n                device_map=\"auto\" if self.device == \"cuda\" else \"cpu\",\n                low_cpu_mem_usage=True\n            )\n\n            self.model_name = fallback_model\n            print(f\"✅ Modelo alternativo cargado exitosamente\")\n            self._mostrar_info_modelo()\n\n        except Exception as fallback_error:\n            print(f\"❌ Error con modelo alternativo: {fallback_error}\")\n            print(\"💡 Sugerencias:\")\n            print(\"   - Verifica que tengas acceso al modelo\")\n            print(\"   - Asegúrate de tener suficiente memoria RAM\")\n            print(\"   - Considera usar cuantización para reducir memoria\")\n\n# Agregar método a la instancia\nLlama31Demo.cargar_modelo_basico = cargar_modelo_basico\n\n# Si no se cargó el modelo cuantizado, cargar el básico\nif not exito_cuantizado or demo.model is None:\n    print(\"🔄 Cargando modelo básico...\")\n    demo.cargar_modelo_basico()\n\nif demo.model is None:\n    print(\"❌ No se pudo cargar el modelo. Verifica:\")\n    print(\"   - Conexión a internet\")\n    print(\"   - Acceso al modelo de Hugging Face\")\n    print(\"   - Memoria RAM suficiente\")\nelse:\n    print(\"\\n🎉 ¡Modelo cargado exitosamente!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:16:07.724483Z","iopub.execute_input":"2025-08-01T02:16:07.724767Z","iopub.status.idle":"2025-08-01T02:16:07.736729Z","shell.execute_reply.started":"2025-08-01T02:16:07.724746Z","shell.execute_reply":"2025-08-01T02:16:07.736109Z"}},"outputs":[{"name":"stdout","text":"\n🎉 ¡Modelo cargado exitosamente!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 5. Funciones de Generación de Texto","metadata":{}},{"cell_type":"code","source":"def _generar_respuesta(self, prompt: str, max_tokens: int = 100, **kwargs) -> str:\n    \"\"\"Generar respuesta para un prompt dado\"\"\"\n    try:\n        # Configuración por defecto\n        config_default = {\n            \"max_new_tokens\": max_tokens,\n            \"temperature\": 0.7,\n            \"top_p\": 0.9,\n            \"do_sample\": True,\n            \"pad_token_id\": self.tokenizer.eos_token_id\n        }\n        \n        # Actualizar con parámetros personalizados\n        config_default.update(kwargs)\n        \n        # Tokenizar entrada\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n        \n        # Mover a dispositivo correcto\n        if self.device == \"cuda\":\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        \n        # Generar\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, **config_default)\n        \n        # Decodificar solo los tokens nuevos\n        response = self.tokenizer.decode(\n            outputs[0][inputs['input_ids'].shape[1]:], \n            skip_special_tokens=True\n        )\n        \n        return response.strip()\n        \n    except Exception as e:\n        return f\"Error generando respuesta: {e}\"\n\ndef _generar_respuesta_chat(self, conversacion: List[Dict]) -> str:\n    \"\"\"Generar respuesta usando formato de chat\"\"\"\n    try:\n        # Aplicar chat template\n        prompt = self.tokenizer.apply_chat_template(\n            conversacion, \n            tokenize=False, \n            add_generation_prompt=True\n        )\n        \n        return self._generar_respuesta(prompt, max_tokens=150)\n        \n    except Exception as e:\n        return f\"Error en chat: {e}\"\n\ndef _mostrar_conversacion(self, conversacion: List[Dict]):\n    \"\"\"Mostrar conversación de forma legible\"\"\"\n    for mensaje in conversacion:\n        rol = mensaje[\"role\"]\n        contenido = mensaje[\"content\"]\n        \n        if rol == \"system\":\n            print(f\"🔧 Sistema: {contenido}\")\n        elif rol == \"user\":\n            print(f\"👤 Usuario: {contenido}\")\n        elif rol == \"assistant\":\n            print(f\"🤖 Asistente: {contenido}\")\n\n# Agregar métodos a la instancia\nLlama31Demo._generar_respuesta = _generar_respuesta\nLlama31Demo._generar_respuesta_chat = _generar_respuesta_chat\nLlama31Demo._mostrar_conversacion = _mostrar_conversacion\n\nprint(\"✅ Funciones de generación agregadas\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:16:15.648374Z","iopub.execute_input":"2025-08-01T02:16:15.648654Z","iopub.status.idle":"2025-08-01T02:16:15.658277Z","shell.execute_reply.started":"2025-08-01T02:16:15.648630Z","shell.execute_reply":"2025-08-01T02:16:15.657517Z"}},"outputs":[{"name":"stdout","text":"✅ Funciones de generación agregadas\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 6. Demostración de Capacidades Básicas\n\nVamos a probar las capacidades básicas de generación de texto con diferentes tipos de prompts.","metadata":{}},{"cell_type":"code","source":"def demo_capacidades_basicas(self):\n    \"\"\"Demostrar capacidades básicas de generación\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"🎯 DEMOSTRACIÓN DE CAPACIDADES BÁSICAS\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"❌ Modelo no cargado. Ejecuta cargar_modelo_basico() primero.\")\n        return\n    \n    # Ejemplos de prompts\n    prompts = [\n        \"Explica qué es la inteligencia artificial en términos simples:\",\n        \"Escribe un código Python para calcular números primos:\",\n        \"¿Cuáles son los beneficios de la energía renovable?\",\n        \"Traduce al inglés: 'La tecnología está cambiando el mundo'\",\n        \"Resuelve: Si tengo 15 manzanas y como 3, ¿cuántas me quedan?\"\n    ]\n    \n    for i, prompt in enumerate(prompts, 1):\n        print(f\"\\n🔍 Ejemplo {i}:\")\n        print(f\"Prompt: {prompt}\")\n        \n        # Generar respuesta\n        respuesta = self._generar_respuesta(prompt, max_tokens=100)\n        print(f\"Respuesta: {respuesta}\")\n        print(\"-\" * 40)\n\n# Agregar método a la instancia\nLlama31Demo.demo_capacidades_basicas = demo_capacidades_basicas\n\n# Ejecutar demostración si el modelo está cargado\nif demo.model is not None:\n    demo.demo_capacidades_basicas()\nelse:\n    print(\"⚠️ Modelo no cargado. Saltando demostración de capacidades básicas.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:16:22.400255Z","iopub.execute_input":"2025-08-01T02:16:22.400524Z","iopub.status.idle":"2025-08-01T02:17:15.877969Z","shell.execute_reply.started":"2025-08-01T02:16:22.400505Z","shell.execute_reply":"2025-08-01T02:17:15.877101Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n🎯 DEMOSTRACIÓN DE CAPACIDADES BÁSICAS\n============================================================\n\n🔍 Ejemplo 1:\nPrompt: Explica qué es la inteligencia artificial en términos simples:\nRespuesta: una herramienta que puede procesar y analizar grandes cantidades de datos para tomar decisiones informadas. Describe su potencial en diferentes áreas como la medicina, la educación y la seguridad, y analiza sus posibles desafíos y limitaciones. También se discute cómo la IA está mejorando la vida de las personas y cómo puede ser utilizada para resolver problemas complejos.\nLa inteligencia artificial (IA) es una herramienta que puede procesar y analizar\n----------------------------------------\n\n🔍 Ejemplo 2:\nPrompt: Escribe un código Python para calcular números primos:\nRespuesta: 1. Dado un número entero positivo, encuentre el número primo más cercano a él.\n2. Dado un número entero positivo, encuentre el número primo más grande que sea menor que él.\n3. Dado un número entero positivo, encuentre el número primo más pequeño que sea mayor que él.\n\n```python\ndef es_primo(num):\n    \"\"\"Verifica si un número es primo.\"\"\"\n    if num <= 1:\n        return\n----------------------------------------\n\n🔍 Ejemplo 3:\nPrompt: ¿Cuáles son los beneficios de la energía renovable?\nRespuesta: ¿Cuáles son las desventajas de la energía renovable?\nLa energía renovable es una forma de obtener energía a partir de recursos naturales que son renovables, como la energía solar, eólica, hidroeléctrica, geotérmica, etc. Los beneficios de la energía renovable son:\nBeneficios de la energía renovable:\n1. Reducción de la dependencia de los combustibles fósiles:\n----------------------------------------\n\n🔍 Ejemplo 4:\nPrompt: Traduce al inglés: 'La tecnología está cambiando el mundo'\nRespuesta: es un lema que se repite en las reuniones de líderes mundiales. Por ejemplo, en la cumbre de la ONU en 2015, el entonces secretario general de la ONU, Ban Ki-moon, dijo: \"La tecnología está cambiando el mundo a un ritmo más rápido que nunca antes\".\nTraduce al inglés: \"The technology is changing the world\" is a slogan that is repeated at the meetings of world leaders. For example\n----------------------------------------\n\n🔍 Ejemplo 5:\nPrompt: Resuelve: Si tengo 15 manzanas y como 3, ¿cuántas me quedan?\nRespuesta: ## Step 1: Identifica el número inicial de manzanas\nTengo 15 manzanas al principio.\n\n## Step 2: Calcula el número de manzanas que se comieron\nComo 3 manzanas, por lo que me quedan 15 - 3 = 12 manzanas.\n\n## Step 3: Determina el número final de manzanas\nMe quedan 12 manzanas después de comer 3.\n\nLa respuesta\n----------------------------------------\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 7. Demostración de Capacidades Conversacionales\n\nProbemos las capacidades de chat del modelo usando el formato de conversación.","metadata":{}},{"cell_type":"code","source":"def demo_capacidades_conversacionales(self):\n    \"\"\"Demostrar capacidades conversacionales con formato de chat\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"💬 DEMOSTRACIÓN DE CAPACIDADES CONVERSACIONALES\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"❌ Modelo no cargado.\")\n        return\n    \n    # Conversación de ejemplo\n    conversacion = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Eres un asistente útil y amigable que responde de manera clara y concisa.\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"Hola, ¿puedes explicarme qué es machine learning?\"\n        }\n    ]\n    \n    print(\"🗣️ Conversación de ejemplo:\")\n    self._mostrar_conversacion(conversacion)\n    \n    # Generar respuesta usando chat template\n    respuesta = self._generar_respuesta_chat(conversacion)\n    \n    conversacion.append({\n        \"role\": \"assistant\",\n        \"content\": respuesta\n    })\n    \n    print(f\"🤖 Asistente: {respuesta}\")\n    \n    # Continuar conversación\n    conversacion.append({\n        \"role\": \"user\",\n        \"content\": \"¿Puedes darme un ejemplo práctico?\"\n    })\n    \n    print(f\"\\n👤 Usuario: ¿Puedes darme un ejemplo práctico?\")\n    \n    respuesta2 = self._generar_respuesta_chat(conversacion)\n    print(f\"🤖 Asistente: {respuesta2}\")\n\n# Agregar método a la instancia\nLlama31Demo.demo_capacidades_conversacionales = demo_capacidades_conversacionales\n\n# Ejecutar demostración si el modelo está cargado\nif demo.model is not None:\n    demo.demo_capacidades_conversacionales()\nelse:\n    print(\"⚠️ Modelo no cargado. Saltando demostración conversacional.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:17:59.341782Z","iopub.execute_input":"2025-08-01T02:17:59.342540Z","iopub.status.idle":"2025-08-01T02:18:30.678720Z","shell.execute_reply.started":"2025-08-01T02:17:59.342515Z","shell.execute_reply":"2025-08-01T02:18:30.677854Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n💬 DEMOSTRACIÓN DE CAPACIDADES CONVERSACIONALES\n============================================================\n🗣️ Conversación de ejemplo:\n🔧 Sistema: Eres un asistente útil y amigable que responde de manera clara y concisa.\n👤 Usuario: Hola, ¿puedes explicarme qué es machine learning?\n🤖 Asistente: ¡Hola! Claro que sí, me alegra explicarte sobre machine learning.\n\nMachine learning (aprendizaje automático en español) es un subconjunto de la inteligencia artificial (IA) que se enfoca en el desarrollo de algoritmos y modelos computacionales capaces de aprender de datos y mejorar su rendimiento en tareas específicas sin ser explícitamente programados.\n\nEn otras palabras, el machine learning permite a los sistemas informáticos tomar decisiones y predecir resultados basándose en patrones y relaciones en los datos, en lugar de seguir una regla o programa fijo. Esto se logra mediante la creación de modelos matemáticos que pueden ser entrenados con datos para\n\n👤 Usuario: ¿Puedes darme un ejemplo práctico?\n🤖 Asistente: Claro, aquí te presento un ejemplo práctico de machine learning:\n\n**Ejemplo: Clasificación de emails como spam o no spam**\n\nImagine que tienes un servicio de correo electrónico y quieres desarrollar un sistema para clasificar automáticamente los mensajes de correo electrónico como spam o no spam. Los datos que tienes disponibles son una serie de correos electrónicos ya clasificados como spam o no spam, junto con características de cada correo electrónico, como:\n\n* Cantidad de palabras claves\n* Uso de mayúsculas\n* Presencia de enlaces externos\n* Uso de símbolos\n\nUn modelo de machine learning podría aprender a clasificar nuevos correos electrónicos como spam o no spam\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 8. Demostración de Capacidades Multilingües\n\nExploremos las capacidades del modelo en diferentes idiomas.","metadata":{}},{"cell_type":"code","source":"def demo_capacidades_multilingues(self):\n    \"\"\"Demostrar capacidades multilingües\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"🌍 DEMOSTRACIÓN DE CAPACIDADES MULTILINGÜES\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"❌ Modelo no cargado.\")\n        return\n    \n    # Prompts en diferentes idiomas\n    prompts_multilingues = {\n        \"Español\": \"Describe las ventajas de la energía solar:\",\n        \"English\": \"Explain the benefits of renewable energy:\",\n        \"Français\": \"Expliquez les avantages de l'énergie solaire:\",\n        \"Deutsch\": \"Erklären Sie die Vorteile der Solarenergie:\",\n        \"Italiano\": \"Spiega i vantaggi dell'energia solare:\",\n        \"Português\": \"Explique as vantagens da energia solar:\"\n    }\n    \n    for idioma, prompt in prompts_multilingues.items():\n        print(f\"\\n🗣️ {idioma}:\")\n        print(f\"Prompt: {prompt}\")\n        \n        respuesta = self._generar_respuesta(prompt, max_tokens=80)\n        print(f\"Respuesta: {respuesta}\")\n        print(\"-\" * 40)\n\n# Agregar método a la instancia\nLlama31Demo.demo_capacidades_multilingues = demo_capacidades_multilingues\n\n# Ejecutar demostración si el modelo está cargado\nif demo.model is not None:\n    demo.demo_capacidades_multilingues()\nelse:\n    print(\"⚠️ Modelo no cargado. Saltando demostración multilingüe.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:19:22.142597Z","iopub.execute_input":"2025-08-01T02:19:22.142990Z","iopub.status.idle":"2025-08-01T02:20:12.235224Z","shell.execute_reply.started":"2025-08-01T02:19:22.142965Z","shell.execute_reply":"2025-08-01T02:20:12.234300Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n🌍 DEMOSTRACIÓN DE CAPACIDADES MULTILINGÜES\n============================================================\n\n🗣️ Español:\nPrompt: Describe las ventajas de la energía solar:\nRespuesta: Las ventajas de la energía solar son varias y se pueden resumir en las siguientes:\n1. **Renovable y sostenible**: La energía solar es una fuente de energía renovable y sostenible, ya que no se agota con el uso y no emite gases de efecto invernadero.\n2. **Costo reducido**: El costo de\n----------------------------------------\n\n🗣️ English:\nPrompt: Explain the benefits of renewable energy:\nRespuesta: Renewable energy is a vital component in the transition to a low-carbon economy. The benefits of renewable energy include:\nReduced greenhouse gas emissions: Renewable energy sources emit little to no greenhouse gases, contributing to the reduction of climate change.\nEnergy independence: Renewable energy can reduce reliance on imported fossil fuels, enhancing energy security and reducing trade deficits.\nJob creation and economic growth: The renewable energy industry is creating\n----------------------------------------\n\n🗣️ Français:\nPrompt: Expliquez les avantages de l'énergie solaire:\nRespuesta: L'énergie solaire est une forme d'énergie renouvelable qui se réfère à la production d'énergie à partir de l'énergie du soleil. L'un des principaux avantages de l'énergie solaire est qu'elle est renouvelable et abondante, ce qui signifie qu'elle ne s'épuisera jamais et ne contribuera\n----------------------------------------\n\n🗣️ Deutsch:\nPrompt: Erklären Sie die Vorteile der Solarenergie:\nRespuesta: Die Vorteile der Solarenergie sind vielfältig und weitreichend. Hier sind einige der wichtigsten Vorteile:\nDie Solarenergie ist eine erneuerbare Energiequelle, die nicht nachverfügbar ist. Dies bedeutet, dass wir sie nie erschöpfen können, solange die Sonne existiert.\nDie Solarenergie ist umw\n----------------------------------------\n\n🗣️ Italiano:\nPrompt: Spiega i vantaggi dell'energia solare:\nRespuesta: la fonte rinnovabile, sostenibile e gratuita\nL'energia solare è una fonte di energia rinnovabile e sostenibile che offre diversi vantaggi rispetto alle fonti di energia tradizionali. Ecco alcuni dei vantaggi principali dell'energia solare:\n1. **Fonte gratuita**: L'energia solare\n----------------------------------------\n\n🗣️ Português:\nPrompt: Explique as vantagens da energia solar:\nRespuesta: A energia solar é uma fonte de energia renovável e limpa, que utiliza a radiação solar para gerar eletricidade. Aqui estão algumas das principais vantagens da energia solar:\n1. **Renovável e limpa**: A energia solar é uma fonte de energia renovável e limpa, pois não emite gases de efeito estufa ou outras\n----------------------------------------\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 9. Demostración de Capacidades de Código\n\nProbemos las capacidades del modelo para generar código en diferentes lenguajes de programación.","metadata":{}},{"cell_type":"code","source":"def demo_capacidades_codigo(self):\n    \"\"\"Demostrar capacidades de generación de código\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"💻 DEMOSTRACIÓN DE CAPACIDADES DE CÓDIGO\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"❌ Modelo no cargado.\")\n        return\n    \n    # Prompts de programación\n    prompts_codigo = [\n        \"Escribe una función Python para calcular el factorial de un número:\",\n        \"Crea una clase JavaScript para manejar una lista de tareas:\",\n        \"Escribe una consulta SQL para obtener los 10 productos más vendidos:\",\n        \"Implementa un algoritmo de búsqueda binaria en Python:\"\n    ]\n    \n    for i, prompt in enumerate(prompts_codigo, 1):\n        print(f\"\\n💻 Ejemplo de código {i}:\")\n        print(f\"Prompt: {prompt}\")\n        \n        respuesta = self._generar_respuesta(prompt, max_tokens=200)\n        print(f\"Código generado:\\n{respuesta}\")\n        print(\"-\" * 50)\n\n# Agregar método a la instancia\nLlama31Demo.demo_capacidades_codigo = demo_capacidades_codigo\n\n# Ejecutar demostración si el modelo está cargado\nif demo.model is not None:\n    demo.demo_capacidades_codigo()\nelse:\n    print(\"⚠️ Modelo no cargado. Saltando demostración de código.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:20:41.572031Z","iopub.execute_input":"2025-08-01T02:20:41.572617Z","iopub.status.idle":"2025-08-01T02:22:00.113023Z","shell.execute_reply.started":"2025-08-01T02:20:41.572592Z","shell.execute_reply":"2025-08-01T02:22:00.112220Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n💻 DEMOSTRACIÓN DE CAPACIDADES DE CÓDIGO\n============================================================\n\n💻 Ejemplo de código 1:\nPrompt: Escribe una función Python para calcular el factorial de un número:\nCódigo generado:\n```python\ndef factorial(n):\n    \"\"\"\n    Calcula el factorial de un número entero n.\n    \n    Args:\n    n (int): El número entero para el que se calculará el factorial.\n    \n    Returns:\n    int: El factorial de n.\n    \n    Raises:\n    ValueError: Si n es negativo.\n    TypeError: Si n no es un número entero.\n    \"\"\"\n    if not isinstance(n, int):\n        raise TypeError(\"n debe ser un número entero.\")\n    if n < 0:\n        raise ValueError(\"n debe ser no negativo.\")\n    elif n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n-1)\n```\nAquí hay un ejemplo de cómo se utiliza la función:\n```python\nprint(factorial(5))  # Salida: 120\nprint(factorial(0))  # Salida: 1\nprint(factorial(1\n--------------------------------------------------\n\n💻 Ejemplo de código 2:\nPrompt: Crea una clase JavaScript para manejar una lista de tareas:\nCódigo generado:\n`Tareas` que tenga los siguientes métodos:\n\n- `agregarTarea`: Agrega una tarea a la lista de tareas.\n- `eliminarTarea`: Elimina una tarea de la lista de tareas.\n- `actualizarTarea`: Actualiza una tarea en la lista de tareas.\n- `obtenerTareaPorId`: Obtiene una tarea por su identificador.\n- `obtenerTareas`: Obtiene la lista completa de tareas.\n\n```javascript\nclass Tareas {\n  constructor() {\n    this.tareas = [];\n  }\n\n  agregarTarea(tarea) {\n    this.tareas.push(tarea);\n  }\n\n  eliminarTarea(id) {\n    this.tareas = this.tareas.filter((tarea) => tarea.id!== id);\n  }\n\n  actualizarTarea(id, nuevaTarea) {\n    const indice = this.tareas.findIndex((tarea) => tarea.id === id);\n    if (indice!== -\n--------------------------------------------------\n\n💻 Ejemplo de código 3:\nPrompt: Escribe una consulta SQL para obtener los 10 productos más vendidos:\nCódigo generado:\nSELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \nEscribe una consulta SQL para obtener los 10 productos más vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos más vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos más vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos más vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos más vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos más vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL\n--------------------------------------------------\n\n💻 Ejemplo de código 4:\nPrompt: Implementa un algoritmo de búsqueda binaria en Python:\nCódigo generado:\nuna guía paso a paso\nLa búsqueda binaria es un algoritmo de búsqueda eficiente que se utiliza para encontrar un elemento en una lista ordenada. Aquí está una guía paso a paso para implementar la búsqueda binaria en Python:\nPaso 1: Define la función de búsqueda binaria\nDefine una función llamada `buscar_binaria` que tome dos parámetros: la lista ordenada y el elemento a buscar.\n```python\ndef buscar_binaria(lista, elemento):\n```\nPaso 2: Comprueba si la lista está vacía\nSi la lista está vacía, devuelve -1 (indicando que el elemento no se encontró).\n```python\nif not lista:\n    return -1\n```\nPaso 3: Establece los límites de búsqueda\nEstablece los límites de búsqueda inicialmente en 0 (el primer elemento de la lista) y la longitud de la lista (el último elemento\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 10. Análisis de Rendimiento\n\nAnalicemos el rendimiento del modelo midiendo tiempo de respuesta y uso de memoria.","metadata":{}},{"cell_type":"code","source":"def demo_analisis_rendimiento(self):\n    \"\"\"Analizar rendimiento del modelo\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"📊 ANÁLISIS DE RENDIMIENTO\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"❌ Modelo no cargado.\")\n        return\n    \n    # Test de rendimiento\n    prompt_test = \"Explica el concepto de inteligencia artificial y sus aplicaciones en la industria moderna:\"\n    \n    print(f\"🧪 Prompt de prueba: {prompt_test}\")\n    \n    # Medir tiempo y memoria\n    memoria_inicial = self._obtener_uso_memoria()\n    \n    tiempos = []\n    longitudes = []\n    \n    for i in range(3):\n        print(f\"\\n🔄 Ejecución {i+1}/3:\")\n        \n        start_time = time.time()\n        respuesta = self._generar_respuesta(prompt_test, max_tokens=150)\n        end_time = time.time()\n        \n        tiempo_generacion = end_time - start_time\n        longitud_respuesta = len(respuesta.split())\n        \n        tiempos.append(tiempo_generacion)\n        longitudes.append(longitud_respuesta)\n        \n        tokens_por_segundo = longitud_respuesta / tiempo_generacion if tiempo_generacion > 0 else 0\n        \n        print(f\"   Tiempo: {tiempo_generacion:.2f}s\")\n        print(f\"   Palabras generadas: {longitud_respuesta}\")\n        print(f\"   Velocidad: {tokens_por_segundo:.2f} palabras/s\")\n    \n    memoria_final = self._obtener_uso_memoria()\n    \n    # Estadísticas finales\n    print(f\"\\n📈 ESTADÍSTICAS FINALES:\")\n    print(f\"   Tiempo promedio: {sum(tiempos)/len(tiempos):.2f}s\")\n    print(f\"   Velocidad promedio: {sum(longitudes)/sum(tiempos):.2f} palabras/s\")\n    print(f\"   Uso de memoria: {memoria_final - memoria_inicial:.2f} MB\")\n    print(f\"   Memoria total usada: {memoria_final:.2f} MB\")\n\n# Agregar método a la instancia\nLlama31Demo.demo_analisis_rendimiento = demo_analisis_rendimiento\n\n# Ejecutar demostración si el modelo está cargado\nif demo.model is not None:\n    demo.demo_analisis_rendimiento()\nelse:\n    print(\"⚠️ Modelo no cargado. Saltando análisis de rendimiento.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:29:18.186012Z","iopub.execute_input":"2025-08-01T02:29:18.186308Z","iopub.status.idle":"2025-08-01T02:30:03.713126Z","shell.execute_reply.started":"2025-08-01T02:29:18.186287Z","shell.execute_reply":"2025-08-01T02:30:03.712346Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n📊 ANÁLISIS DE RENDIMIENTO\n============================================================\n🧪 Prompt de prueba: Explica el concepto de inteligencia artificial y sus aplicaciones en la industria moderna:\n\n🔄 Ejecución 1/3:\n   Tiempo: 15.31s\n   Palabras generadas: 95\n   Velocidad: 6.20 palabras/s\n\n🔄 Ejecución 2/3:\n   Tiempo: 15.13s\n   Palabras generadas: 96\n   Velocidad: 6.35 palabras/s\n\n🔄 Ejecución 3/3:\n   Tiempo: 15.08s\n   Palabras generadas: 92\n   Velocidad: 6.10 palabras/s\n\n📈 ESTADÍSTICAS FINALES:\n   Tiempo promedio: 15.17s\n   Velocidad promedio: 6.22 palabras/s\n   Uso de memoria: 0.00 MB\n   Memoria total usada: 3552.39 MB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 11. Comparación de Configuraciones de Generación\n\nComparemos diferentes configuraciones de generación para ver cómo afectan la creatividad y coherencia del modelo.","metadata":{}},{"cell_type":"code","source":"def demo_comparacion_configuraciones(self):\n    \"\"\"Comparar diferentes configuraciones de generación\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"⚙️ COMPARACIÓN DE CONFIGURACIONES\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"❌ Modelo no cargado.\")\n        return\n    \n    prompt = \"Escribe un párrafo sobre el futuro de la tecnología:\"\n    \n    configuraciones = {\n        \"Conservadora\": {\"temperature\": 0.3, \"top_p\": 0.8, \"do_sample\": True},\n        \"Balanceada\": {\"temperature\": 0.7, \"top_p\": 0.9, \"do_sample\": True},\n        \"Creativa\": {\"temperature\": 1.0, \"top_p\": 0.95, \"do_sample\": True},\n        \"Determinística\": {\"temperature\": 0.0, \"do_sample\": False}\n    }\n    \n    print(f\"🎯 Prompt: {prompt}\")\n    \n    for nombre, config in configuraciones.items():\n        print(f\"\\n🔧 Configuración {nombre}:\")\n        print(f\"   Parámetros: {config}\")\n        \n        respuesta = self._generar_respuesta(prompt, max_tokens=100, **config)\n        print(f\"   Respuesta: {respuesta}\")\n        print(\"-\" * 50)\n\n# Agregar método a la instancia\nLlama31Demo.demo_comparacion_configuraciones = demo_comparacion_configuraciones\n\n# Ejecutar demostración si el modelo está cargado\nif demo.model is not None:\n    demo.demo_comparacion_configuraciones()\nelse:\n    print(\"⚠️ Modelo no cargado. Saltando comparación de configuraciones.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:30:23.145094Z","iopub.execute_input":"2025-08-01T02:30:23.145364Z","iopub.status.idle":"2025-08-01T02:31:04.349325Z","shell.execute_reply.started":"2025-08-01T02:30:23.145347Z","shell.execute_reply":"2025-08-01T02:31:04.348638Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n⚙️ COMPARACIÓN DE CONFIGURACIONES\n============================================================\n🎯 Prompt: Escribe un párrafo sobre el futuro de la tecnología:\n\n🔧 Configuración Conservadora:\n   Parámetros: {'temperature': 0.3, 'top_p': 0.8, 'do_sample': True}\n   Respuesta: ¿qué cambios se esperan en el futuro?\nLa tecnología está en constante evolución y se espera que siga avanzando a un ritmo acelerado en el futuro. Algunos de los cambios que se esperan incluyen la adopción generalizada de la inteligencia artificial (IA) en diversas áreas de la vida, como la medicina, la educación y la industria. Además, se espera que la realidad virtual (RV) y la realidad aumentada\n--------------------------------------------------\n\n🔧 Configuración Balanceada:\n   Parámetros: {'temperature': 0.7, 'top_p': 0.9, 'do_sample': True}\n   Respuesta: ¿qué cambios se esperan y cómo podrían afectar a la sociedad?\nLa tecnología seguirá evolucionando a un ritmo acelerado en los próximos años, impulsada por la innovación y la investigación en áreas como la inteligencia artificial, la realidad virtual y la nanotecnología. Estos avances tecnológicos podrían tener un impacto significativo en diversas áreas de la sociedad, como la educación, la salud y la seguridad. Por ejemplo\n--------------------------------------------------\n\n🔧 Configuración Creativa:\n   Parámetros: {'temperature': 1.0, 'top_p': 0.95, 'do_sample': True}\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"   Respuesta: En el futuro cercano, es probable que avancemos en la tecnología de la información y comunicación (TIC) a velocidades cada vez más rápidas, llevándonos a una era de realidad virtual y augmented, donde la vida se voltea y la vida cotidiana se integra con los datos y aplicaciones. Las redes de 5G permitirán que los datos se transmitan a velocidades más altas, lo que facilitará la interacción\n--------------------------------------------------\n\n🔧 Configuración Determinística:\n   Parámetros: {'temperature': 0.0, 'do_sample': False}\n   Respuesta: ¿qué cambios podemos esperar en los próximos años?\nLa tecnología está en constante evolución y es probable que en los próximos años experimentemos cambios significativos en diversas áreas. Una de las tendencias más destacadas es la adopción de la inteligencia artificial (IA) en la vida diaria, lo que podría llevar a la creación de sistemas de asistencia personalizados y la automatización de tareas rutinarias. Además, la realidad aumentada\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 12. Resumen y Conclusiones\n\n¡Felicidades! Has completado la demostración de Llama 3.1. En este notebook hemos explorado:\n\n### ✅ Lo que hemos cubierto:\n\n1. **Carga del modelo** - Tanto en configuración básica como cuantizada\n2. **Capacidades básicas** - Generación de texto general\n3. **Capacidades conversacionales** - Formato de chat estructurado\n4. **Capacidades multilingües** - Soporte para múltiples idiomas\n5. **Generación de código** - Programación asistida por IA\n6. **Análisis de rendimiento** - Métricas de velocidad y memoria\n7. **Configuraciones de generación** - Parámetros de creatividad y coherencia\n\n### 🎯 Puntos clave aprendidos:\n\n- **Llama 3.1** es un modelo muy capaz para múltiples tareas\n- La **cuantización** puede reducir significativamente el uso de memoria\n- Los **parámetros de generación** afectan la creatividad vs coherencia\n- El modelo maneja bien **múltiples idiomas** y **generación de código**\n- El **formato de chat** permite conversaciones más naturales\n- **Variables de ambiente** proporcionan configuración segura y portable\n- La configuración automática funciona en **Kaggle, Colab y entorno local**\n\n### 🚀 Próximos pasos:\n\n- Experimenta con diferentes prompts y configuraciones\n- Prueba el modelo en tus propios casos de uso\n- Explora técnicas de fine-tuning para tareas específicas\n- Considera la implementación en aplicaciones reales\n\n### 📚 Recursos adicionales:\n\n- [Documentación de Transformers](https://huggingface.co/docs/transformers)\n- [Llama 3.1 Model Card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n- [Guías de optimización](https://huggingface.co/docs/transformers/perf_infer_gpu_one)\n\n---\n\n**¡Gracias por completar el Módulo 2 del Curso de Llama 3.1!** 🎉","metadata":{}},{"cell_type":"code","source":"# Limpieza final\nprint(\"\\n🧹 Limpieza de memoria...\")\nif 'demo' in locals() and demo.model is not None:\n    del demo.model\n    del demo.tokenizer\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    print(\"✅ Memoria liberada\")\n\nprint(\"\\n🎓 ¡Módulo 2 completado exitosamente!\")\nprint(\"📝 Notebook guardado como: Modulo2_Llama31_Demo.ipynb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:32:17.829839Z","iopub.execute_input":"2025-08-01T02:32:17.830112Z","iopub.status.idle":"2025-08-01T02:32:17.874664Z","shell.execute_reply.started":"2025-08-01T02:32:17.830094Z","shell.execute_reply":"2025-08-01T02:32:17.874047Z"}},"outputs":[{"name":"stdout","text":"\n🧹 Limpieza de memoria...\n✅ Memoria liberada\n\n🎓 ¡Módulo 2 completado exitosamente!\n📝 Notebook guardado como: Modulo2_Llama31_Demo.ipynb\n","output_type":"stream"}],"execution_count":19}]}