{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# M√≥dulo 2: Introducci√≥n a Llama 3.1\n\n## Demostraci√≥n pr√°ctica de las capacidades y caracter√≠sticas de Llama 3.1\n\n**Autor:** Pablo √Ålvarez \n**Fecha:** Julio 2025\n\n---\n\n### üöÄ **Compatible con Kaggle**\n\nEste notebook est√° optimizado para ejecutarse en:\n- **Kaggle Notebooks** (recomendado para GPU gratuita)\n- **Google Colab**\n- **Entorno local**\n\n### üîë **Configuraci√≥n de Token HF (Variables de Ambiente):**\n\nEste notebook lee el token de Hugging Face directamente desde las **variables de ambiente del sistema operativo**:\n\n**Para Kaggle:**\n1. Ve a **Settings > Secrets** en tu notebook\n2. Agrega `HF_TOKEN` con tu token de Hugging Face\n3. El notebook lo detectar√° autom√°ticamente\n\n**Para Google Colab:**\n1. Ejecuta: `import os`\n2. Ejecuta: `os.environ['HF_TOKEN'] = 'tu_token_aqui'`\n\n**Para entorno local:**\n1. `export HF_TOKEN=tu_token_aqui`\n2. O agrega al archivo `.bashrc`/`.zshrc`\n\n**Obtener token:** https://huggingface.co/settings/tokens\n\n### üöÄ **Activar GPU (Kaggle):**\n- Ve a **Settings > Accelerator**\n- Selecciona **GPU T4 x2** (gratuito)\n\n---\n\nEn este notebook exploraremos las capacidades de Llama 3.1, incluyendo:\n\n1. **Carga del modelo** - Configuraci√≥n b√°sica y cuantizada\n2. **Capacidades b√°sicas** - Generaci√≥n de texto general\n3. **Capacidades conversacionales** - Formato de chat\n4. **Capacidades multiling√ºes** - Soporte para m√∫ltiples idiomas\n5. **Generaci√≥n de c√≥digo** - Programaci√≥n asistida por IA\n6. **An√°lisis de rendimiento** - M√©tricas y optimizaci√≥n\n7. **Comparaci√≥n de configuraciones** - Diferentes par√°metros de generaci√≥n","metadata":{}},{"cell_type":"markdown","source":"## 1. Verificaci√≥n del Entorno y Configuraci√≥n","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:09:51.710107Z","iopub.execute_input":"2025-08-01T02:09:51.710345Z","iopub.status.idle":"2025-08-01T02:11:12.398736Z","shell.execute_reply.started":"2025-08-01T02:09:51.710328Z","shell.execute_reply":"2025-08-01T02:11:12.397839Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Verificar entorno y configuraci√≥n\nimport os\nimport sys\n\n# Detectar entorno\ndef detectar_entorno():\n    if os.path.exists('/kaggle/input') or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n        return 'Kaggle'\n    elif 'COLAB_GPU' in os.environ:\n        return 'Google Colab'\n    else:\n        return 'Local'\n\nentorno = detectar_entorno()\nprint(f\"üîç Entorno detectado: {entorno}\")\n\nfrom kaggle_secrets import UserSecretsClient\n\n# Accede el secreto \"HF_TOKEN\"\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n# (Opcional) lo setea como variable de entorno para que Transformers lo tome autom√°ticamente\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token  # A veces esta variable tambi√©n es requerida\n\nprint(\"Token HF seteado correctamente:\", hf_token[:10], \"...\")\n\n# Verificar GPU\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"üöÄ GPU disponible: {gpu_name}\")\n    print(f\"üíæ Memoria GPU: {gpu_memory:.1f} GB\")\nelse:\n    print(\"‚ö†Ô∏è GPU no disponible - usando CPU\")\n\nprint(f\"üêç Python: {sys.version}\")\nprint(f\"üî• PyTorch: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:15.915723Z","iopub.execute_input":"2025-08-01T02:12:15.916078Z","iopub.status.idle":"2025-08-01T02:12:17.812848Z","shell.execute_reply.started":"2025-08-01T02:12:15.916048Z","shell.execute_reply":"2025-08-01T02:12:17.812109Z"}},"outputs":[{"name":"stdout","text":"üîç Entorno detectado: Kaggle\nToken HF seteado correctamente: hf_FLjugcm ...\nüöÄ GPU disponible: Tesla T4\nüíæ Memoria GPU: 14.7 GB\nüêç Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nüî• PyTorch: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 2. Configuraci√≥n Manual del Token (Opcional)\n\nSi necesitas configurar el token manualmente en el notebook, ejecuta la siguiente celda:","metadata":{}},{"cell_type":"code","source":"# OPCIONAL: Configurar token manualmente si no est√° en variables de ambiente\n# Descomenta y ejecuta solo si es necesario\n\n# import os\n# \n# # Reemplaza 'tu_token_aqui' con tu token real de Hugging Face\n# # os.environ['HF_TOKEN'] = 'tu_token_aqui'\n# \n# # Verificar que se configur√≥ correctamente\n# if os.getenv('HF_TOKEN'):\n#     print(f\"‚úÖ Token configurado: {os.getenv('HF_TOKEN')[:10]}...\")\n# else:\n#     print(\"‚ö†Ô∏è Token no configurado\")\n\nprint(\"üí° Esta celda es opcional. Solo √∫sala si el token no se detecta autom√°ticamente.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:22.080181Z","iopub.execute_input":"2025-08-01T02:12:22.080840Z","iopub.status.idle":"2025-08-01T02:12:22.084727Z","shell.execute_reply.started":"2025-08-01T02:12:22.080800Z","shell.execute_reply":"2025-08-01T02:12:22.084084Z"}},"outputs":[{"name":"stdout","text":"üí° Esta celda es opcional. Solo √∫sala si el token no se detecta autom√°ticamente.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Importaciones y Configuraci√≥n Inicial","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\nimport torch\nimport time\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import List, Dict, Optional\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Detectar entorno\ndef is_kaggle():\n    return os.path.exists('/kaggle/input') or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n\ndef is_colab():\n    return 'COLAB_GPU' in os.environ\n\n# Configurar paths seg√∫n el entorno\nif is_kaggle():\n    print(\"üîç Ejecutando en Kaggle\")\n    # En Kaggle, el c√≥digo puede estar en /kaggle/input/\n    config_paths = ['/kaggle/input/cursollama311/config', './config', '../config']\nelif is_colab():\n    print(\"üîç Ejecutando en Google Colab\")\n    config_paths = ['./config', '../config']\nelse:\n    print(\"üîç Ejecutando en entorno local\")\n    config_paths = ['./config', '../config', str(Path.cwd().parent / 'config')]\n\n# Agregar paths de configuraci√≥n\nfor path in config_paths:\n    if os.path.exists(path):\n        sys.path.insert(0, path)\n        break\n\n# Instalar dependencias si es necesario (para Kaggle/Colab)\nif is_kaggle() or is_colab():\n    try:\n        import transformers\n        print(f\"‚úÖ Transformers {transformers.__version__} ya instalado\")\n    except ImportError:\n        print(\"üì¶ Instalando transformers...\")\n        %pip install -q transformers accelerate bitsandbytes\n\n# Importar librer√≠as principales\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    pipeline\n)\n\n# Importar psutil con fallback\ntry:\n    import psutil\n    HAS_PSUTIL = True\nexcept ImportError:\n    print(\"‚ö†Ô∏è psutil no disponible - m√©tricas de memoria limitadas\")\n    HAS_PSUTIL = False\n\n# Importar matplotlib con fallback\ntry:\n    import matplotlib.pyplot as plt\n    HAS_MATPLOTLIB = True\nexcept ImportError:\n    print(\"‚ö†Ô∏è matplotlib no disponible - sin gr√°ficos\")\n    HAS_MATPLOTLIB = False\n\n# Configuraci√≥n simplificada usando variables de ambiente del sistema\ndef setup_huggingface_auth():\n    \"\"\"Configurar autenticaci√≥n usando variables de ambiente del sistema operativo\"\"\"\n    # Buscar token en m√∫ltiples variables de ambiente\n    token = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n    \n    if token:\n        # Configurar todas las variables que transformers puede usar\n        os.environ['HUGGING_FACE_HUB_TOKEN'] = token\n        os.environ['HF_TOKEN'] = token\n        print(\"üîê Autenticaci√≥n HF configurada desde variables de ambiente del sistema\")\n        return True\n    else:\n        print(\"‚ö†Ô∏è No se encontr√≥ token HF en variables de ambiente del sistema\")\n        return False\n\ndef get_model_config():\n    \"\"\"Obtener configuraci√≥n de modelo desde variables de ambiente\"\"\"\n    return {\n        'default_model': os.getenv('DEFAULT_MODEL', 'meta-llama/Meta-Llama-3.1-8B-Instruct'),\n        'fallback_model': os.getenv('FALLBACK_MODEL', 'microsoft/DialoGPT-medium'),\n        'cache_dir': os.getenv('HF_CACHE_DIR', '/tmp/huggingface_cache' if is_kaggle() else './models_cache'),\n        'device': os.getenv('DEVICE', 'auto'),\n        'use_quantization': os.getenv('USE_QUANTIZATION', 'true').lower() == 'true'\n    }\n\n# Configurar autenticaci√≥n autom√°ticamente al importar\nauth_success = setup_huggingface_auth()\nmodel_config = get_model_config()\n\nprint(f\"üîß Configuraci√≥n desde variables de ambiente:\")\nprint(f\"   Autenticaci√≥n: {'‚úÖ Configurada' if auth_success else '‚ùå No configurada'}\")\nprint(f\"   Modelo: {model_config['default_model']}\")\nprint(f\"   Cache: {model_config['cache_dir']}\")\nprint(f\"   Cuantizaci√≥n: {model_config['use_quantization']}\")\n\nprint(\"üì¶ Librer√≠as importadas correctamente\")\nprint(f\"üî• PyTorch: {torch.__version__}\")\nprint(f\"ü§ó Transformers: {transformers.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:25.922708Z","iopub.execute_input":"2025-08-01T02:12:25.923290Z","iopub.status.idle":"2025-08-01T02:12:48.450487Z","shell.execute_reply.started":"2025-08-01T02:12:25.923265Z","shell.execute_reply":"2025-08-01T02:12:48.449786Z"}},"outputs":[{"name":"stdout","text":"üîç Ejecutando en Kaggle\n‚úÖ Transformers 4.52.4 ya instalado\n","output_type":"stream"},{"name":"stderr","text":"2025-08-01 02:12:34.755450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754014354.969193      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754014355.031275      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"üîê Autenticaci√≥n HF configurada desde variables de ambiente del sistema\nüîß Configuraci√≥n desde variables de ambiente:\n   Autenticaci√≥n: ‚úÖ Configurada\n   Modelo: meta-llama/Meta-Llama-3.1-8B-Instruct\n   Cache: /tmp/huggingface_cache\n   Cuantizaci√≥n: True\nüì¶ Librer√≠as importadas correctamente\nüî• PyTorch: 2.6.0+cu124\nü§ó Transformers: 4.52.4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2. Definici√≥n de la Clase Llama31Demo","metadata":{}},{"cell_type":"code","source":"class Llama31Demo:\n    \"\"\"Clase para demostrar las capacidades de Llama 3.1\"\"\"\n    \n    def __init__(self, model_name: str = None):\n        # Usar configuraci√≥n desde variables de ambiente del sistema\n        config = get_model_config()\n        self.model_name = model_name or config['default_model']\n        self.cache_dir = config['cache_dir']\n        self.use_quantization = config['use_quantization']\n        \n        self.tokenizer = None\n        self.model = None\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        print(f\"ü¶ô Inicializando demostraci√≥n de Llama 3.1\")\n        print(f\"üì± Dispositivo: {self.device}\")\n        print(f\"ü§ñ Modelo: {self.model_name}\")\n        print(f\"üìÅ Cache: {self.cache_dir}\")\n        print(f\"‚ö° Cuantizaci√≥n: {self.use_quantization}\")\n    \n    def _obtener_uso_memoria(self) -> float:\n        \"\"\"Obtener uso actual de memoria en MB\"\"\"\n        process = psutil.Process(os.getpid())\n        return process.memory_info().rss / (1024 * 1024)\n    \n    def _mostrar_info_modelo(self):\n        \"\"\"Mostrar informaci√≥n detallada del modelo\"\"\"\n        if self.model is None:\n            return\n        \n        # Contar par√°metros\n        total_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        \n        print(f\"\\nüìä Informaci√≥n del modelo:\")\n        print(f\"   Nombre: {self.model_name}\")\n        print(f\"   Par√°metros totales: {total_params:,}\")\n        print(f\"   Par√°metros entrenables: {trainable_params:,}\")\n        print(f\"   Dispositivo: {next(self.model.parameters()).device}\")\n        print(f\"   Tipo de datos: {next(self.model.parameters()).dtype}\")\n        \n        # Uso de memoria\n        memoria_mb = self._obtener_uso_memoria()\n        print(f\"   Memoria utilizada: {memoria_mb:.2f} MB\")\n\n# Crear instancia de la demo\ndemo = Llama31Demo()\nprint(\"‚úÖ Instancia de Llama31Demo creada\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:48.499170Z","iopub.execute_input":"2025-08-01T02:12:48.499456Z","iopub.status.idle":"2025-08-01T02:12:48.513153Z","shell.execute_reply.started":"2025-08-01T02:12:48.499433Z","shell.execute_reply":"2025-08-01T02:12:48.512590Z"}},"outputs":[{"name":"stdout","text":"ü¶ô Inicializando demostraci√≥n de Llama 3.1\nüì± Dispositivo: cuda\nü§ñ Modelo: meta-llama/Meta-Llama-3.1-8B-Instruct\nüìÅ Cache: /tmp/huggingface_cache\n‚ö° Cuantizaci√≥n: True\n‚úÖ Instancia de Llama31Demo creada\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 3. Carga del Modelo - Configuraci√≥n Cuantizada\n\nPrimero intentaremos cargar el modelo con cuantizaci√≥n 4-bit para optimizar el uso de memoria.","metadata":{}},{"cell_type":"code","source":"def cargar_modelo_cuantizado(self):\n    \"\"\"Cargar Llama 3.1 con cuantizaci√≥n para eficiencia\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"‚ö° CARGANDO LLAMA 3.1 - CONFIGURACI√ìN CUANTIZADA\")\n    print(\"=\"*60)\n    \n    try:\n        # Configuraci√≥n de cuantizaci√≥n\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n        \n        print(\"üîß Configuraci√≥n de cuantizaci√≥n 4-bit activada\")\n        \n        # Cargar tokenizer usando token desde variables de ambiente\n        if self.tokenizer is None:\n            hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.model_name,\n                token=hf_token,\n                cache_dir=self.cache_dir\n            )\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Cargar modelo cuantizado\n        print(\"üß† Cargando modelo cuantizado...\")\n        start_time = time.time()\n        \n        hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            token=hf_token,\n            cache_dir=self.cache_dir\n        )\n        \n        load_time = time.time() - start_time\n        \n        print(f\"‚úÖ Modelo cuantizado cargado en {load_time:.2f} segundos\")\n        print(\"üíæ Uso de memoria significativamente reducido\")\n        \n        # Mostrar informaci√≥n\n        self._mostrar_info_modelo()\n        \n    except Exception as e:\n        print(f\"‚ùå Error cargando modelo cuantizado: {e}\")\n        if \"bitsandbytes\" in str(e):\n            print(\"üí° Instalaci√≥n requerida: pip install bitsandbytes\")\n        print(\"üîÑ Cargando modelo b√°sico...\")\n        return False\n    return True\n\n# Agregar m√©todo a la instancia\nLlama31Demo.cargar_modelo_cuantizado = cargar_modelo_cuantizado\n\n# Intentar cargar modelo cuantizado\nprint(\"üîÑ Intentando cargar modelo cuantizado...\")\nexito_cuantizado = demo.cargar_modelo_cuantizado()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:12:53.815291Z","iopub.execute_input":"2025-08-01T02:12:53.815536Z","iopub.status.idle":"2025-08-01T02:15:54.591765Z","shell.execute_reply.started":"2025-08-01T02:12:53.815519Z","shell.execute_reply":"2025-08-01T02:15:54.591104Z"}},"outputs":[{"name":"stdout","text":"üîÑ Intentando cargar modelo cuantizado...\n\n============================================================\n‚ö° CARGANDO LLAMA 3.1 - CONFIGURACI√ìN CUANTIZADA\n============================================================\nüîß Configuraci√≥n de cuantizaci√≥n 4-bit activada\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fa8c97cb69f4ac5a9bebde2c24657ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551d166803b64c819267e92d758b28b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71bfd097f4664d64bb4abc5d5bf2dd07"}},"metadata":{}},{"name":"stdout","text":"üß† Cargando modelo cuantizado...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23dfb0bf3ca6497988a3e2e889cb65c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d392ac66b4040659e268d4ad8c4554c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbf1d712953405696e83dab985358e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8d41ea06564432be41ae7a0d61cf02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27687907a0d649c8ace528b02814ab43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8e9e6de3dd4386bd195575b5f4dab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6478f687b6b44cab39f4601bf482620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"781390b034ee48bcb5e6f5dcd55cb799"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5701cf7ae42e4172910a72da8435bd2c"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Modelo cuantizado cargado en 178.42 segundos\nüíæ Uso de memoria significativamente reducido\n\nüìä Informaci√≥n del modelo:\n   Nombre: meta-llama/Meta-Llama-3.1-8B-Instruct\n   Par√°metros totales: 4,540,600,320\n   Par√°metros entrenables: 1,050,939,392\n   Dispositivo: cuda:0\n   Tipo de datos: torch.float16\n   Memoria utilizada: 3277.27 MB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 4. Carga del Modelo - Configuraci√≥n B√°sica\n\nSi la cuantizaci√≥n falla, cargaremos el modelo en configuraci√≥n b√°sica.","metadata":{}},{"cell_type":"code","source":"def cargar_modelo_basico(self):\n    \"\"\"Cargar Llama 3.1 en configuraci√≥n b√°sica\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üì• CARGANDO LLAMA 3.1 - CONFIGURACI√ìN B√ÅSICA\")\n    print(\"=\"*60)\n    \n    try:\n        # Cargar tokenizer usando token desde variables de ambiente\n        print(\"üî§ Cargando tokenizer...\")\n        hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_name,\n            token=hf_token,\n            cache_dir=self.cache_dir\n        )\n        \n        # Configurar pad token si no existe\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        print(f\"‚úÖ Tokenizer cargado\")\n        print(f\"   Vocabulario: {self.tokenizer.vocab_size:,} tokens\")\n        print(f\"   Tokens especiales: {len(self.tokenizer.special_tokens_map)}\")\n        \n        # Cargar modelo\n        print(\"üß† Cargando modelo...\")\n        start_time = time.time()\n        \n        hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN')\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n            device_map=\"auto\" if self.device == \"cuda\" else \"cpu\",\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            token=hf_token,\n            cache_dir=self.cache_dir\n        )\n        \n        load_time = time.time() - start_time\n        \n        print(f\"‚úÖ Modelo cargado en {load_time:.2f} segundos\")\n        \n        # Informaci√≥n del modelo\n        self._mostrar_info_modelo()\n        \n    except Exception as e:\n        print(f\"‚ùå Error cargando modelo: {e}\")\n        print(\"üí° Intentando con modelo alternativo...\")\n\n        # Intentar con modelo alternativo\n        try:\n            fallback_model = \"microsoft/DialoGPT-medium\"\n            print(f\"üîÑ Cargando modelo alternativo: {fallback_model}\")\n\n            self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n\n            self.model = AutoModelForCausalLM.from_pretrained(\n                fallback_model,\n                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n                device_map=\"auto\" if self.device == \"cuda\" else \"cpu\",\n                low_cpu_mem_usage=True\n            )\n\n            self.model_name = fallback_model\n            print(f\"‚úÖ Modelo alternativo cargado exitosamente\")\n            self._mostrar_info_modelo()\n\n        except Exception as fallback_error:\n            print(f\"‚ùå Error con modelo alternativo: {fallback_error}\")\n            print(\"üí° Sugerencias:\")\n            print(\"   - Verifica que tengas acceso al modelo\")\n            print(\"   - Aseg√∫rate de tener suficiente memoria RAM\")\n            print(\"   - Considera usar cuantizaci√≥n para reducir memoria\")\n\n# Agregar m√©todo a la instancia\nLlama31Demo.cargar_modelo_basico = cargar_modelo_basico\n\n# Si no se carg√≥ el modelo cuantizado, cargar el b√°sico\nif not exito_cuantizado or demo.model is None:\n    print(\"üîÑ Cargando modelo b√°sico...\")\n    demo.cargar_modelo_basico()\n\nif demo.model is None:\n    print(\"‚ùå No se pudo cargar el modelo. Verifica:\")\n    print(\"   - Conexi√≥n a internet\")\n    print(\"   - Acceso al modelo de Hugging Face\")\n    print(\"   - Memoria RAM suficiente\")\nelse:\n    print(\"\\nüéâ ¬°Modelo cargado exitosamente!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:16:07.724483Z","iopub.execute_input":"2025-08-01T02:16:07.724767Z","iopub.status.idle":"2025-08-01T02:16:07.736729Z","shell.execute_reply.started":"2025-08-01T02:16:07.724746Z","shell.execute_reply":"2025-08-01T02:16:07.736109Z"}},"outputs":[{"name":"stdout","text":"\nüéâ ¬°Modelo cargado exitosamente!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 5. Funciones de Generaci√≥n de Texto","metadata":{}},{"cell_type":"code","source":"def _generar_respuesta(self, prompt: str, max_tokens: int = 100, **kwargs) -> str:\n    \"\"\"Generar respuesta para un prompt dado\"\"\"\n    try:\n        # Configuraci√≥n por defecto\n        config_default = {\n            \"max_new_tokens\": max_tokens,\n            \"temperature\": 0.7,\n            \"top_p\": 0.9,\n            \"do_sample\": True,\n            \"pad_token_id\": self.tokenizer.eos_token_id\n        }\n        \n        # Actualizar con par√°metros personalizados\n        config_default.update(kwargs)\n        \n        # Tokenizar entrada\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n        \n        # Mover a dispositivo correcto\n        if self.device == \"cuda\":\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        \n        # Generar\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, **config_default)\n        \n        # Decodificar solo los tokens nuevos\n        response = self.tokenizer.decode(\n            outputs[0][inputs['input_ids'].shape[1]:], \n            skip_special_tokens=True\n        )\n        \n        return response.strip()\n        \n    except Exception as e:\n        return f\"Error generando respuesta: {e}\"\n\ndef _generar_respuesta_chat(self, conversacion: List[Dict]) -> str:\n    \"\"\"Generar respuesta usando formato de chat\"\"\"\n    try:\n        # Aplicar chat template\n        prompt = self.tokenizer.apply_chat_template(\n            conversacion, \n            tokenize=False, \n            add_generation_prompt=True\n        )\n        \n        return self._generar_respuesta(prompt, max_tokens=150)\n        \n    except Exception as e:\n        return f\"Error en chat: {e}\"\n\ndef _mostrar_conversacion(self, conversacion: List[Dict]):\n    \"\"\"Mostrar conversaci√≥n de forma legible\"\"\"\n    for mensaje in conversacion:\n        rol = mensaje[\"role\"]\n        contenido = mensaje[\"content\"]\n        \n        if rol == \"system\":\n            print(f\"üîß Sistema: {contenido}\")\n        elif rol == \"user\":\n            print(f\"üë§ Usuario: {contenido}\")\n        elif rol == \"assistant\":\n            print(f\"ü§ñ Asistente: {contenido}\")\n\n# Agregar m√©todos a la instancia\nLlama31Demo._generar_respuesta = _generar_respuesta\nLlama31Demo._generar_respuesta_chat = _generar_respuesta_chat\nLlama31Demo._mostrar_conversacion = _mostrar_conversacion\n\nprint(\"‚úÖ Funciones de generaci√≥n agregadas\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:16:15.648374Z","iopub.execute_input":"2025-08-01T02:16:15.648654Z","iopub.status.idle":"2025-08-01T02:16:15.658277Z","shell.execute_reply.started":"2025-08-01T02:16:15.648630Z","shell.execute_reply":"2025-08-01T02:16:15.657517Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Funciones de generaci√≥n agregadas\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 6. Demostraci√≥n de Capacidades B√°sicas\n\nVamos a probar las capacidades b√°sicas de generaci√≥n de texto con diferentes tipos de prompts.","metadata":{}},{"cell_type":"code","source":"def demo_capacidades_basicas(self):\n    \"\"\"Demostrar capacidades b√°sicas de generaci√≥n\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üéØ DEMOSTRACI√ìN DE CAPACIDADES B√ÅSICAS\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"‚ùå Modelo no cargado. Ejecuta cargar_modelo_basico() primero.\")\n        return\n    \n    # Ejemplos de prompts\n    prompts = [\n        \"Explica qu√© es la inteligencia artificial en t√©rminos simples:\",\n        \"Escribe un c√≥digo Python para calcular n√∫meros primos:\",\n        \"¬øCu√°les son los beneficios de la energ√≠a renovable?\",\n        \"Traduce al ingl√©s: 'La tecnolog√≠a est√° cambiando el mundo'\",\n        \"Resuelve: Si tengo 15 manzanas y como 3, ¬øcu√°ntas me quedan?\"\n    ]\n    \n    for i, prompt in enumerate(prompts, 1):\n        print(f\"\\nüîç Ejemplo {i}:\")\n        print(f\"Prompt: {prompt}\")\n        \n        # Generar respuesta\n        respuesta = self._generar_respuesta(prompt, max_tokens=100)\n        print(f\"Respuesta: {respuesta}\")\n        print(\"-\" * 40)\n\n# Agregar m√©todo a la instancia\nLlama31Demo.demo_capacidades_basicas = demo_capacidades_basicas\n\n# Ejecutar demostraci√≥n si el modelo est√° cargado\nif demo.model is not None:\n    demo.demo_capacidades_basicas()\nelse:\n    print(\"‚ö†Ô∏è Modelo no cargado. Saltando demostraci√≥n de capacidades b√°sicas.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:16:22.400255Z","iopub.execute_input":"2025-08-01T02:16:22.400524Z","iopub.status.idle":"2025-08-01T02:17:15.877969Z","shell.execute_reply.started":"2025-08-01T02:16:22.400505Z","shell.execute_reply":"2025-08-01T02:17:15.877101Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüéØ DEMOSTRACI√ìN DE CAPACIDADES B√ÅSICAS\n============================================================\n\nüîç Ejemplo 1:\nPrompt: Explica qu√© es la inteligencia artificial en t√©rminos simples:\nRespuesta: una herramienta que puede procesar y analizar grandes cantidades de datos para tomar decisiones informadas. Describe su potencial en diferentes √°reas como la medicina, la educaci√≥n y la seguridad, y analiza sus posibles desaf√≠os y limitaciones. Tambi√©n se discute c√≥mo la IA est√° mejorando la vida de las personas y c√≥mo puede ser utilizada para resolver problemas complejos.\nLa inteligencia artificial (IA) es una herramienta que puede procesar y analizar\n----------------------------------------\n\nüîç Ejemplo 2:\nPrompt: Escribe un c√≥digo Python para calcular n√∫meros primos:\nRespuesta: 1. Dado un n√∫mero entero positivo, encuentre el n√∫mero primo m√°s cercano a √©l.\n2. Dado un n√∫mero entero positivo, encuentre el n√∫mero primo m√°s grande que sea menor que √©l.\n3. Dado un n√∫mero entero positivo, encuentre el n√∫mero primo m√°s peque√±o que sea mayor que √©l.\n\n```python\ndef es_primo(num):\n    \"\"\"Verifica si un n√∫mero es primo.\"\"\"\n    if num <= 1:\n        return\n----------------------------------------\n\nüîç Ejemplo 3:\nPrompt: ¬øCu√°les son los beneficios de la energ√≠a renovable?\nRespuesta: ¬øCu√°les son las desventajas de la energ√≠a renovable?\nLa energ√≠a renovable es una forma de obtener energ√≠a a partir de recursos naturales que son renovables, como la energ√≠a solar, e√≥lica, hidroel√©ctrica, geot√©rmica, etc. Los beneficios de la energ√≠a renovable son:\nBeneficios de la energ√≠a renovable:\n1. Reducci√≥n de la dependencia de los combustibles f√≥siles:\n----------------------------------------\n\nüîç Ejemplo 4:\nPrompt: Traduce al ingl√©s: 'La tecnolog√≠a est√° cambiando el mundo'\nRespuesta: es un lema que se repite en las reuniones de l√≠deres mundiales. Por ejemplo, en la cumbre de la ONU en 2015, el entonces secretario general de la ONU, Ban Ki-moon, dijo: \"La tecnolog√≠a est√° cambiando el mundo a un ritmo m√°s r√°pido que nunca antes\".\nTraduce al ingl√©s: \"The technology is changing the world\" is a slogan that is repeated at the meetings of world leaders. For example\n----------------------------------------\n\nüîç Ejemplo 5:\nPrompt: Resuelve: Si tengo 15 manzanas y como 3, ¬øcu√°ntas me quedan?\nRespuesta: ## Step 1: Identifica el n√∫mero inicial de manzanas\nTengo 15 manzanas al principio.\n\n## Step 2: Calcula el n√∫mero de manzanas que se comieron\nComo 3 manzanas, por lo que me quedan 15 - 3 = 12 manzanas.\n\n## Step 3: Determina el n√∫mero final de manzanas\nMe quedan 12 manzanas despu√©s de comer 3.\n\nLa respuesta\n----------------------------------------\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 7. Demostraci√≥n de Capacidades Conversacionales\n\nProbemos las capacidades de chat del modelo usando el formato de conversaci√≥n.","metadata":{}},{"cell_type":"code","source":"def demo_capacidades_conversacionales(self):\n    \"\"\"Demostrar capacidades conversacionales con formato de chat\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üí¨ DEMOSTRACI√ìN DE CAPACIDADES CONVERSACIONALES\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"‚ùå Modelo no cargado.\")\n        return\n    \n    # Conversaci√≥n de ejemplo\n    conversacion = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Eres un asistente √∫til y amigable que responde de manera clara y concisa.\"\n        },\n        {\n            \"role\": \"user\", \n            \"content\": \"Hola, ¬øpuedes explicarme qu√© es machine learning?\"\n        }\n    ]\n    \n    print(\"üó£Ô∏è Conversaci√≥n de ejemplo:\")\n    self._mostrar_conversacion(conversacion)\n    \n    # Generar respuesta usando chat template\n    respuesta = self._generar_respuesta_chat(conversacion)\n    \n    conversacion.append({\n        \"role\": \"assistant\",\n        \"content\": respuesta\n    })\n    \n    print(f\"ü§ñ Asistente: {respuesta}\")\n    \n    # Continuar conversaci√≥n\n    conversacion.append({\n        \"role\": \"user\",\n        \"content\": \"¬øPuedes darme un ejemplo pr√°ctico?\"\n    })\n    \n    print(f\"\\nüë§ Usuario: ¬øPuedes darme un ejemplo pr√°ctico?\")\n    \n    respuesta2 = self._generar_respuesta_chat(conversacion)\n    print(f\"ü§ñ Asistente: {respuesta2}\")\n\n# Agregar m√©todo a la instancia\nLlama31Demo.demo_capacidades_conversacionales = demo_capacidades_conversacionales\n\n# Ejecutar demostraci√≥n si el modelo est√° cargado\nif demo.model is not None:\n    demo.demo_capacidades_conversacionales()\nelse:\n    print(\"‚ö†Ô∏è Modelo no cargado. Saltando demostraci√≥n conversacional.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:17:59.341782Z","iopub.execute_input":"2025-08-01T02:17:59.342540Z","iopub.status.idle":"2025-08-01T02:18:30.678720Z","shell.execute_reply.started":"2025-08-01T02:17:59.342515Z","shell.execute_reply":"2025-08-01T02:18:30.677854Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüí¨ DEMOSTRACI√ìN DE CAPACIDADES CONVERSACIONALES\n============================================================\nüó£Ô∏è Conversaci√≥n de ejemplo:\nüîß Sistema: Eres un asistente √∫til y amigable que responde de manera clara y concisa.\nüë§ Usuario: Hola, ¬øpuedes explicarme qu√© es machine learning?\nü§ñ Asistente: ¬°Hola! Claro que s√≠, me alegra explicarte sobre machine learning.\n\nMachine learning (aprendizaje autom√°tico en espa√±ol) es un subconjunto de la inteligencia artificial (IA) que se enfoca en el desarrollo de algoritmos y modelos computacionales capaces de aprender de datos y mejorar su rendimiento en tareas espec√≠ficas sin ser expl√≠citamente programados.\n\nEn otras palabras, el machine learning permite a los sistemas inform√°ticos tomar decisiones y predecir resultados bas√°ndose en patrones y relaciones en los datos, en lugar de seguir una regla o programa fijo. Esto se logra mediante la creaci√≥n de modelos matem√°ticos que pueden ser entrenados con datos para\n\nüë§ Usuario: ¬øPuedes darme un ejemplo pr√°ctico?\nü§ñ Asistente: Claro, aqu√≠ te presento un ejemplo pr√°ctico de machine learning:\n\n**Ejemplo: Clasificaci√≥n de emails como spam o no spam**\n\nImagine que tienes un servicio de correo electr√≥nico y quieres desarrollar un sistema para clasificar autom√°ticamente los mensajes de correo electr√≥nico como spam o no spam. Los datos que tienes disponibles son una serie de correos electr√≥nicos ya clasificados como spam o no spam, junto con caracter√≠sticas de cada correo electr√≥nico, como:\n\n* Cantidad de palabras claves\n* Uso de may√∫sculas\n* Presencia de enlaces externos\n* Uso de s√≠mbolos\n\nUn modelo de machine learning podr√≠a aprender a clasificar nuevos correos electr√≥nicos como spam o no spam\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 8. Demostraci√≥n de Capacidades Multiling√ºes\n\nExploremos las capacidades del modelo en diferentes idiomas.","metadata":{}},{"cell_type":"code","source":"def demo_capacidades_multilingues(self):\n    \"\"\"Demostrar capacidades multiling√ºes\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üåç DEMOSTRACI√ìN DE CAPACIDADES MULTILING√úES\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"‚ùå Modelo no cargado.\")\n        return\n    \n    # Prompts en diferentes idiomas\n    prompts_multilingues = {\n        \"Espa√±ol\": \"Describe las ventajas de la energ√≠a solar:\",\n        \"English\": \"Explain the benefits of renewable energy:\",\n        \"Fran√ßais\": \"Expliquez les avantages de l'√©nergie solaire:\",\n        \"Deutsch\": \"Erkl√§ren Sie die Vorteile der Solarenergie:\",\n        \"Italiano\": \"Spiega i vantaggi dell'energia solare:\",\n        \"Portugu√™s\": \"Explique as vantagens da energia solar:\"\n    }\n    \n    for idioma, prompt in prompts_multilingues.items():\n        print(f\"\\nüó£Ô∏è {idioma}:\")\n        print(f\"Prompt: {prompt}\")\n        \n        respuesta = self._generar_respuesta(prompt, max_tokens=80)\n        print(f\"Respuesta: {respuesta}\")\n        print(\"-\" * 40)\n\n# Agregar m√©todo a la instancia\nLlama31Demo.demo_capacidades_multilingues = demo_capacidades_multilingues\n\n# Ejecutar demostraci√≥n si el modelo est√° cargado\nif demo.model is not None:\n    demo.demo_capacidades_multilingues()\nelse:\n    print(\"‚ö†Ô∏è Modelo no cargado. Saltando demostraci√≥n multiling√ºe.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:19:22.142597Z","iopub.execute_input":"2025-08-01T02:19:22.142990Z","iopub.status.idle":"2025-08-01T02:20:12.235224Z","shell.execute_reply.started":"2025-08-01T02:19:22.142965Z","shell.execute_reply":"2025-08-01T02:20:12.234300Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüåç DEMOSTRACI√ìN DE CAPACIDADES MULTILING√úES\n============================================================\n\nüó£Ô∏è Espa√±ol:\nPrompt: Describe las ventajas de la energ√≠a solar:\nRespuesta: Las ventajas de la energ√≠a solar son varias y se pueden resumir en las siguientes:\n1. **Renovable y sostenible**: La energ√≠a solar es una fuente de energ√≠a renovable y sostenible, ya que no se agota con el uso y no emite gases de efecto invernadero.\n2. **Costo reducido**: El costo de\n----------------------------------------\n\nüó£Ô∏è English:\nPrompt: Explain the benefits of renewable energy:\nRespuesta: Renewable energy is a vital component in the transition to a low-carbon economy. The benefits of renewable energy include:\nReduced greenhouse gas emissions: Renewable energy sources emit little to no greenhouse gases, contributing to the reduction of climate change.\nEnergy independence: Renewable energy can reduce reliance on imported fossil fuels, enhancing energy security and reducing trade deficits.\nJob creation and economic growth: The renewable energy industry is creating\n----------------------------------------\n\nüó£Ô∏è Fran√ßais:\nPrompt: Expliquez les avantages de l'√©nergie solaire:\nRespuesta: L'√©nergie solaire est une forme d'√©nergie renouvelable qui se r√©f√®re √† la production d'√©nergie √† partir de l'√©nergie du soleil. L'un des principaux avantages de l'√©nergie solaire est qu'elle est renouvelable et abondante, ce qui signifie qu'elle ne s'√©puisera jamais et ne contribuera\n----------------------------------------\n\nüó£Ô∏è Deutsch:\nPrompt: Erkl√§ren Sie die Vorteile der Solarenergie:\nRespuesta: Die Vorteile der Solarenergie sind vielf√§ltig und weitreichend. Hier sind einige der wichtigsten Vorteile:\nDie Solarenergie ist eine erneuerbare Energiequelle, die nicht nachverf√ºgbar ist. Dies bedeutet, dass wir sie nie ersch√∂pfen k√∂nnen, solange die Sonne existiert.\nDie Solarenergie ist umw\n----------------------------------------\n\nüó£Ô∏è Italiano:\nPrompt: Spiega i vantaggi dell'energia solare:\nRespuesta: la fonte rinnovabile, sostenibile e gratuita\nL'energia solare √® una fonte di energia rinnovabile e sostenibile che offre diversi vantaggi rispetto alle fonti di energia tradizionali. Ecco alcuni dei vantaggi principali dell'energia solare:\n1. **Fonte gratuita**: L'energia solare\n----------------------------------------\n\nüó£Ô∏è Portugu√™s:\nPrompt: Explique as vantagens da energia solar:\nRespuesta: A energia solar √© uma fonte de energia renov√°vel e limpa, que utiliza a radia√ß√£o solar para gerar eletricidade. Aqui est√£o algumas das principais vantagens da energia solar:\n1. **Renov√°vel e limpa**: A energia solar √© uma fonte de energia renov√°vel e limpa, pois n√£o emite gases de efeito estufa ou outras\n----------------------------------------\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 9. Demostraci√≥n de Capacidades de C√≥digo\n\nProbemos las capacidades del modelo para generar c√≥digo en diferentes lenguajes de programaci√≥n.","metadata":{}},{"cell_type":"code","source":"def demo_capacidades_codigo(self):\n    \"\"\"Demostrar capacidades de generaci√≥n de c√≥digo\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üíª DEMOSTRACI√ìN DE CAPACIDADES DE C√ìDIGO\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"‚ùå Modelo no cargado.\")\n        return\n    \n    # Prompts de programaci√≥n\n    prompts_codigo = [\n        \"Escribe una funci√≥n Python para calcular el factorial de un n√∫mero:\",\n        \"Crea una clase JavaScript para manejar una lista de tareas:\",\n        \"Escribe una consulta SQL para obtener los 10 productos m√°s vendidos:\",\n        \"Implementa un algoritmo de b√∫squeda binaria en Python:\"\n    ]\n    \n    for i, prompt in enumerate(prompts_codigo, 1):\n        print(f\"\\nüíª Ejemplo de c√≥digo {i}:\")\n        print(f\"Prompt: {prompt}\")\n        \n        respuesta = self._generar_respuesta(prompt, max_tokens=200)\n        print(f\"C√≥digo generado:\\n{respuesta}\")\n        print(\"-\" * 50)\n\n# Agregar m√©todo a la instancia\nLlama31Demo.demo_capacidades_codigo = demo_capacidades_codigo\n\n# Ejecutar demostraci√≥n si el modelo est√° cargado\nif demo.model is not None:\n    demo.demo_capacidades_codigo()\nelse:\n    print(\"‚ö†Ô∏è Modelo no cargado. Saltando demostraci√≥n de c√≥digo.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:20:41.572031Z","iopub.execute_input":"2025-08-01T02:20:41.572617Z","iopub.status.idle":"2025-08-01T02:22:00.113023Z","shell.execute_reply.started":"2025-08-01T02:20:41.572592Z","shell.execute_reply":"2025-08-01T02:22:00.112220Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüíª DEMOSTRACI√ìN DE CAPACIDADES DE C√ìDIGO\n============================================================\n\nüíª Ejemplo de c√≥digo 1:\nPrompt: Escribe una funci√≥n Python para calcular el factorial de un n√∫mero:\nC√≥digo generado:\n```python\ndef factorial(n):\n    \"\"\"\n    Calcula el factorial de un n√∫mero entero n.\n    \n    Args:\n    n (int): El n√∫mero entero para el que se calcular√° el factorial.\n    \n    Returns:\n    int: El factorial de n.\n    \n    Raises:\n    ValueError: Si n es negativo.\n    TypeError: Si n no es un n√∫mero entero.\n    \"\"\"\n    if not isinstance(n, int):\n        raise TypeError(\"n debe ser un n√∫mero entero.\")\n    if n < 0:\n        raise ValueError(\"n debe ser no negativo.\")\n    elif n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n-1)\n```\nAqu√≠ hay un ejemplo de c√≥mo se utiliza la funci√≥n:\n```python\nprint(factorial(5))  # Salida: 120\nprint(factorial(0))  # Salida: 1\nprint(factorial(1\n--------------------------------------------------\n\nüíª Ejemplo de c√≥digo 2:\nPrompt: Crea una clase JavaScript para manejar una lista de tareas:\nC√≥digo generado:\n`Tareas` que tenga los siguientes m√©todos:\n\n- `agregarTarea`: Agrega una tarea a la lista de tareas.\n- `eliminarTarea`: Elimina una tarea de la lista de tareas.\n- `actualizarTarea`: Actualiza una tarea en la lista de tareas.\n- `obtenerTareaPorId`: Obtiene una tarea por su identificador.\n- `obtenerTareas`: Obtiene la lista completa de tareas.\n\n```javascript\nclass Tareas {\n  constructor() {\n    this.tareas = [];\n  }\n\n  agregarTarea(tarea) {\n    this.tareas.push(tarea);\n  }\n\n  eliminarTarea(id) {\n    this.tareas = this.tareas.filter((tarea) => tarea.id!== id);\n  }\n\n  actualizarTarea(id, nuevaTarea) {\n    const indice = this.tareas.findIndex((tarea) => tarea.id === id);\n    if (indice!== -\n--------------------------------------------------\n\nüíª Ejemplo de c√≥digo 3:\nPrompt: Escribe una consulta SQL para obtener los 10 productos m√°s vendidos:\nC√≥digo generado:\nSELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \nEscribe una consulta SQL para obtener los 10 productos m√°s vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos m√°s vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos m√°s vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos m√°s vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos m√°s vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL para obtener los 10 productos m√°s vendidos: SELECT nombre, cantidad FROM productos ORDER BY cantidad DESC LIMIT 10; \n\nEscribe una consulta SQL\n--------------------------------------------------\n\nüíª Ejemplo de c√≥digo 4:\nPrompt: Implementa un algoritmo de b√∫squeda binaria en Python:\nC√≥digo generado:\nuna gu√≠a paso a paso\nLa b√∫squeda binaria es un algoritmo de b√∫squeda eficiente que se utiliza para encontrar un elemento en una lista ordenada. Aqu√≠ est√° una gu√≠a paso a paso para implementar la b√∫squeda binaria en Python:\nPaso 1: Define la funci√≥n de b√∫squeda binaria\nDefine una funci√≥n llamada `buscar_binaria` que tome dos par√°metros: la lista ordenada y el elemento a buscar.\n```python\ndef buscar_binaria(lista, elemento):\n```\nPaso 2: Comprueba si la lista est√° vac√≠a\nSi la lista est√° vac√≠a, devuelve -1 (indicando que el elemento no se encontr√≥).\n```python\nif not lista:\n    return -1\n```\nPaso 3: Establece los l√≠mites de b√∫squeda\nEstablece los l√≠mites de b√∫squeda inicialmente en 0 (el primer elemento de la lista) y la longitud de la lista (el √∫ltimo elemento\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 10. An√°lisis de Rendimiento\n\nAnalicemos el rendimiento del modelo midiendo tiempo de respuesta y uso de memoria.","metadata":{}},{"cell_type":"code","source":"def demo_analisis_rendimiento(self):\n    \"\"\"Analizar rendimiento del modelo\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üìä AN√ÅLISIS DE RENDIMIENTO\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"‚ùå Modelo no cargado.\")\n        return\n    \n    # Test de rendimiento\n    prompt_test = \"Explica el concepto de inteligencia artificial y sus aplicaciones en la industria moderna:\"\n    \n    print(f\"üß™ Prompt de prueba: {prompt_test}\")\n    \n    # Medir tiempo y memoria\n    memoria_inicial = self._obtener_uso_memoria()\n    \n    tiempos = []\n    longitudes = []\n    \n    for i in range(3):\n        print(f\"\\nüîÑ Ejecuci√≥n {i+1}/3:\")\n        \n        start_time = time.time()\n        respuesta = self._generar_respuesta(prompt_test, max_tokens=150)\n        end_time = time.time()\n        \n        tiempo_generacion = end_time - start_time\n        longitud_respuesta = len(respuesta.split())\n        \n        tiempos.append(tiempo_generacion)\n        longitudes.append(longitud_respuesta)\n        \n        tokens_por_segundo = longitud_respuesta / tiempo_generacion if tiempo_generacion > 0 else 0\n        \n        print(f\"   Tiempo: {tiempo_generacion:.2f}s\")\n        print(f\"   Palabras generadas: {longitud_respuesta}\")\n        print(f\"   Velocidad: {tokens_por_segundo:.2f} palabras/s\")\n    \n    memoria_final = self._obtener_uso_memoria()\n    \n    # Estad√≠sticas finales\n    print(f\"\\nüìà ESTAD√çSTICAS FINALES:\")\n    print(f\"   Tiempo promedio: {sum(tiempos)/len(tiempos):.2f}s\")\n    print(f\"   Velocidad promedio: {sum(longitudes)/sum(tiempos):.2f} palabras/s\")\n    print(f\"   Uso de memoria: {memoria_final - memoria_inicial:.2f} MB\")\n    print(f\"   Memoria total usada: {memoria_final:.2f} MB\")\n\n# Agregar m√©todo a la instancia\nLlama31Demo.demo_analisis_rendimiento = demo_analisis_rendimiento\n\n# Ejecutar demostraci√≥n si el modelo est√° cargado\nif demo.model is not None:\n    demo.demo_analisis_rendimiento()\nelse:\n    print(\"‚ö†Ô∏è Modelo no cargado. Saltando an√°lisis de rendimiento.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:29:18.186012Z","iopub.execute_input":"2025-08-01T02:29:18.186308Z","iopub.status.idle":"2025-08-01T02:30:03.713126Z","shell.execute_reply.started":"2025-08-01T02:29:18.186287Z","shell.execute_reply":"2025-08-01T02:30:03.712346Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüìä AN√ÅLISIS DE RENDIMIENTO\n============================================================\nüß™ Prompt de prueba: Explica el concepto de inteligencia artificial y sus aplicaciones en la industria moderna:\n\nüîÑ Ejecuci√≥n 1/3:\n   Tiempo: 15.31s\n   Palabras generadas: 95\n   Velocidad: 6.20 palabras/s\n\nüîÑ Ejecuci√≥n 2/3:\n   Tiempo: 15.13s\n   Palabras generadas: 96\n   Velocidad: 6.35 palabras/s\n\nüîÑ Ejecuci√≥n 3/3:\n   Tiempo: 15.08s\n   Palabras generadas: 92\n   Velocidad: 6.10 palabras/s\n\nüìà ESTAD√çSTICAS FINALES:\n   Tiempo promedio: 15.17s\n   Velocidad promedio: 6.22 palabras/s\n   Uso de memoria: 0.00 MB\n   Memoria total usada: 3552.39 MB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 11. Comparaci√≥n de Configuraciones de Generaci√≥n\n\nComparemos diferentes configuraciones de generaci√≥n para ver c√≥mo afectan la creatividad y coherencia del modelo.","metadata":{}},{"cell_type":"code","source":"def demo_comparacion_configuraciones(self):\n    \"\"\"Comparar diferentes configuraciones de generaci√≥n\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"‚öôÔ∏è COMPARACI√ìN DE CONFIGURACIONES\")\n    print(\"=\"*60)\n    \n    if self.model is None or self.tokenizer is None:\n        print(\"‚ùå Modelo no cargado.\")\n        return\n    \n    prompt = \"Escribe un p√°rrafo sobre el futuro de la tecnolog√≠a:\"\n    \n    configuraciones = {\n        \"Conservadora\": {\"temperature\": 0.3, \"top_p\": 0.8, \"do_sample\": True},\n        \"Balanceada\": {\"temperature\": 0.7, \"top_p\": 0.9, \"do_sample\": True},\n        \"Creativa\": {\"temperature\": 1.0, \"top_p\": 0.95, \"do_sample\": True},\n        \"Determin√≠stica\": {\"temperature\": 0.0, \"do_sample\": False}\n    }\n    \n    print(f\"üéØ Prompt: {prompt}\")\n    \n    for nombre, config in configuraciones.items():\n        print(f\"\\nüîß Configuraci√≥n {nombre}:\")\n        print(f\"   Par√°metros: {config}\")\n        \n        respuesta = self._generar_respuesta(prompt, max_tokens=100, **config)\n        print(f\"   Respuesta: {respuesta}\")\n        print(\"-\" * 50)\n\n# Agregar m√©todo a la instancia\nLlama31Demo.demo_comparacion_configuraciones = demo_comparacion_configuraciones\n\n# Ejecutar demostraci√≥n si el modelo est√° cargado\nif demo.model is not None:\n    demo.demo_comparacion_configuraciones()\nelse:\n    print(\"‚ö†Ô∏è Modelo no cargado. Saltando comparaci√≥n de configuraciones.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:30:23.145094Z","iopub.execute_input":"2025-08-01T02:30:23.145364Z","iopub.status.idle":"2025-08-01T02:31:04.349325Z","shell.execute_reply.started":"2025-08-01T02:30:23.145347Z","shell.execute_reply":"2025-08-01T02:31:04.348638Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n‚öôÔ∏è COMPARACI√ìN DE CONFIGURACIONES\n============================================================\nüéØ Prompt: Escribe un p√°rrafo sobre el futuro de la tecnolog√≠a:\n\nüîß Configuraci√≥n Conservadora:\n   Par√°metros: {'temperature': 0.3, 'top_p': 0.8, 'do_sample': True}\n   Respuesta: ¬øqu√© cambios se esperan en el futuro?\nLa tecnolog√≠a est√° en constante evoluci√≥n y se espera que siga avanzando a un ritmo acelerado en el futuro. Algunos de los cambios que se esperan incluyen la adopci√≥n generalizada de la inteligencia artificial (IA) en diversas √°reas de la vida, como la medicina, la educaci√≥n y la industria. Adem√°s, se espera que la realidad virtual (RV) y la realidad aumentada\n--------------------------------------------------\n\nüîß Configuraci√≥n Balanceada:\n   Par√°metros: {'temperature': 0.7, 'top_p': 0.9, 'do_sample': True}\n   Respuesta: ¬øqu√© cambios se esperan y c√≥mo podr√≠an afectar a la sociedad?\nLa tecnolog√≠a seguir√° evolucionando a un ritmo acelerado en los pr√≥ximos a√±os, impulsada por la innovaci√≥n y la investigaci√≥n en √°reas como la inteligencia artificial, la realidad virtual y la nanotecnolog√≠a. Estos avances tecnol√≥gicos podr√≠an tener un impacto significativo en diversas √°reas de la sociedad, como la educaci√≥n, la salud y la seguridad. Por ejemplo\n--------------------------------------------------\n\nüîß Configuraci√≥n Creativa:\n   Par√°metros: {'temperature': 1.0, 'top_p': 0.95, 'do_sample': True}\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"   Respuesta: En el futuro cercano, es probable que avancemos en la tecnolog√≠a de la informaci√≥n y comunicaci√≥n (TIC) a velocidades cada vez m√°s r√°pidas, llev√°ndonos a una era de realidad virtual y augmented, donde la vida se voltea y la vida cotidiana se integra con los datos y aplicaciones. Las redes de 5G permitir√°n que los datos se transmitan a velocidades m√°s altas, lo que facilitar√° la interacci√≥n\n--------------------------------------------------\n\nüîß Configuraci√≥n Determin√≠stica:\n   Par√°metros: {'temperature': 0.0, 'do_sample': False}\n   Respuesta: ¬øqu√© cambios podemos esperar en los pr√≥ximos a√±os?\nLa tecnolog√≠a est√° en constante evoluci√≥n y es probable que en los pr√≥ximos a√±os experimentemos cambios significativos en diversas √°reas. Una de las tendencias m√°s destacadas es la adopci√≥n de la inteligencia artificial (IA) en la vida diaria, lo que podr√≠a llevar a la creaci√≥n de sistemas de asistencia personalizados y la automatizaci√≥n de tareas rutinarias. Adem√°s, la realidad aumentada\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 12. Resumen y Conclusiones\n\n¬°Felicidades! Has completado la demostraci√≥n de Llama 3.1. En este notebook hemos explorado:\n\n### ‚úÖ Lo que hemos cubierto:\n\n1. **Carga del modelo** - Tanto en configuraci√≥n b√°sica como cuantizada\n2. **Capacidades b√°sicas** - Generaci√≥n de texto general\n3. **Capacidades conversacionales** - Formato de chat estructurado\n4. **Capacidades multiling√ºes** - Soporte para m√∫ltiples idiomas\n5. **Generaci√≥n de c√≥digo** - Programaci√≥n asistida por IA\n6. **An√°lisis de rendimiento** - M√©tricas de velocidad y memoria\n7. **Configuraciones de generaci√≥n** - Par√°metros de creatividad y coherencia\n\n### üéØ Puntos clave aprendidos:\n\n- **Llama 3.1** es un modelo muy capaz para m√∫ltiples tareas\n- La **cuantizaci√≥n** puede reducir significativamente el uso de memoria\n- Los **par√°metros de generaci√≥n** afectan la creatividad vs coherencia\n- El modelo maneja bien **m√∫ltiples idiomas** y **generaci√≥n de c√≥digo**\n- El **formato de chat** permite conversaciones m√°s naturales\n- **Variables de ambiente** proporcionan configuraci√≥n segura y portable\n- La configuraci√≥n autom√°tica funciona en **Kaggle, Colab y entorno local**\n\n### üöÄ Pr√≥ximos pasos:\n\n- Experimenta con diferentes prompts y configuraciones\n- Prueba el modelo en tus propios casos de uso\n- Explora t√©cnicas de fine-tuning para tareas espec√≠ficas\n- Considera la implementaci√≥n en aplicaciones reales\n\n### üìö Recursos adicionales:\n\n- [Documentaci√≥n de Transformers](https://huggingface.co/docs/transformers)\n- [Llama 3.1 Model Card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)\n- [Gu√≠as de optimizaci√≥n](https://huggingface.co/docs/transformers/perf_infer_gpu_one)\n\n---\n\n**¬°Gracias por completar el M√≥dulo 2 del Curso de Llama 3.1!** üéâ","metadata":{}},{"cell_type":"code","source":"# Limpieza final\nprint(\"\\nüßπ Limpieza de memoria...\")\nif 'demo' in locals() and demo.model is not None:\n    del demo.model\n    del demo.tokenizer\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    print(\"‚úÖ Memoria liberada\")\n\nprint(\"\\nüéì ¬°M√≥dulo 2 completado exitosamente!\")\nprint(\"üìù Notebook guardado como: Modulo2_Llama31_Demo.ipynb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T02:32:17.829839Z","iopub.execute_input":"2025-08-01T02:32:17.830112Z","iopub.status.idle":"2025-08-01T02:32:17.874664Z","shell.execute_reply.started":"2025-08-01T02:32:17.830094Z","shell.execute_reply":"2025-08-01T02:32:17.874047Z"}},"outputs":[{"name":"stdout","text":"\nüßπ Limpieza de memoria...\n‚úÖ Memoria liberada\n\nüéì ¬°M√≥dulo 2 completado exitosamente!\nüìù Notebook guardado como: Modulo2_Llama31_Demo.ipynb\n","output_type":"stream"}],"execution_count":19}]}