# MÃ³dulo 1 â€“ Fundamentos y AtenciÃ³n

## Contenido
- **ğŸ¯ demo_atencion.ipynb**: **ACTUALIZADO** - DemostraciÃ³n interactiva avanzada de mecanismos de atenciÃ³n con visualizaciones mejoradas
- laboratorio-modulo-n1.ipynb: Ejercicios prÃ¡cticos de fundamentos y atenciÃ³n

## Objetivos
- Entender el mecanismo de atenciÃ³n a alto nivel
- Manipular parÃ¡metros simples y observar su impacto
- **ğŸ†• Visualizar matrices de atenciÃ³n** con grÃ¡ficos interactivos
- **ğŸ†• Explorar diferentes capas y cabezas** de atenciÃ³n
- **ğŸ†• Analizar patrones de atenciÃ³n** en modelos BERT en espaÃ±ol

## ğŸ†• Nuevas CaracterÃ­sticas del Demo de AtenciÃ³n

### ğŸ¨ Visualizaciones Avanzadas:
- **ğŸ“Š GrÃ¡ficos de barras interactivos** - Visualizar scores de atenciÃ³n por token
- **ğŸ”— Diagramas de lÃ­neas de atenciÃ³n** - Conexiones visuales entre tokens
- **ğŸ¯ VisualizaciÃ³n ultra-wide** - Formato optimizado para anÃ¡lisis detallado
- **ğŸŒˆ Mapas de calor con colores** - Intensidad de atenciÃ³n con escala de colores

### ğŸ›ï¸ Controles Interactivos:
- **Selector de capas** - Explorar diferentes niveles del modelo
- **Selector de cabezas** - Analizar mÃºltiples cabezas de atenciÃ³n
- **Selector de tokens** - Investigar atenciÃ³n desde cualquier token
- **Umbral configurable** - Filtrar conexiones de baja intensidad

### ğŸ“ˆ Funcionalidades TÃ©cnicas:
- **ExtracciÃ³n de matrices de atenciÃ³n** de modelos BERT
- **AnÃ¡lisis cuantitativo** con valores numÃ©ricos precisos
- **NormalizaciÃ³n automÃ¡tica** de scores de atenciÃ³n
- **Resaltado inteligente** de tokens origen y destino mÃ¡s relevantes

## Requisitos
- Python 3.10+
- Jupyter Lab/Notebook
- **ğŸ†• Widgets interactivos** para controles avanzados

## EjecuciÃ³n rÃ¡pida

### ğŸ¯ Demo de AtenciÃ³n Interactivo (RECOMENDADO):
1. Abre `demo_atencion.ipynb`
2. **CaracterÃ­sticas**: Visualizaciones interactivas, mÃºltiples tipos de grÃ¡ficos, controles de widgets
3. **Modelos**: BERT en espaÃ±ol para anÃ¡lisis de atenciÃ³n
4. **Tiempo estimado**: 10-15 minutos para exploraciÃ³n completa

### ğŸ“š Laboratorio PrÃ¡ctico:
1. Abre `laboratorio-modulo-n1.ipynb`
2. Ejercicios paso a paso sobre fundamentos
3. Tiempo estimado: 20-30 minutos

## InstalaciÃ³n de Dependencias

```bash
# Dependencias bÃ¡sicas
pip install torch transformers

# Visualizaciones avanzadas
pip install matplotlib seaborn

# Widgets interactivos (NUEVO)
pip install ipywidgets

# Para Jupyter Lab (si usas JupyterLab)
jupyter labextension install @jupyter-widgets/jupyterlab-manager
```

## ğŸ® CÃ³mo Usar las Nuevas Funcionalidades

### 1. VisualizaciÃ³n BÃ¡sica de AtenciÃ³n:
- Ejecuta las celdas hasta llegar a los widgets
- Usa los sliders para cambiar capa, cabeza y token
- Observa cÃ³mo cambian los patrones de atenciÃ³n

### 2. AnÃ¡lisis Avanzado:
- **plot_attention()**: GrÃ¡fico de barras tradicional
- **plot_attention_lines_ultra_wide()**: VisualizaciÃ³n con lÃ­neas de conexiÃ³n
- Ajusta el parÃ¡metro `thresh` para filtrar conexiones dÃ©biles

### 3. InterpretaciÃ³n de Resultados:
- **LÃ­neas gruesas**: Mayor atenciÃ³n entre tokens
- **Colores intensos**: Scores de atenciÃ³n altos
- **Tokens resaltados**: Origen (azul) y destino principal (rojo)
- **Valores numÃ©ricos**: Scores exactos de atenciÃ³n

## ğŸ’¡ Tips para el AnÃ¡lisis:
- Prueba diferentes capas (0-11) para ver evoluciÃ³n de patrones
- Compara mÃºltiples cabezas en la misma capa
- Analiza tokens de funciÃ³n vs. tokens de contenido
- Observa patrones de auto-atenciÃ³n vs. atenciÃ³n cruzada

## ğŸ” Casos de Uso Educativos:
- **Demostrar conceptos** de atenciÃ³n en transformers
- **Analizar comportamiento** de modelos pre-entrenados
- **Comparar estrategias** de diferentes cabezas de atenciÃ³n
- **Entender relaciones** sintÃ¡cticas y semÃ¡nticas

